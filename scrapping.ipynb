{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A faire : \n",
    "- indexer les articles (et indice inversé)\n",
    "- changer les noms des fichiers json pour qu'ils soient appelés par leur titre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install wikipedia-api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wikipediaapi\n",
    "\n",
    "\n",
    "wiki_wiki = wikipediaapi.Wikipedia(\n",
    "    user_agent='MyProjectName',\n",
    "        language='en',\n",
    "        extract_format=wikipediaapi.ExtractFormat.WIKI\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Get data for each category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import shuffle\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def url_categorymembers(categorymembers,list_pages=[], level=0, max_level=3, i=0):\n",
    "    pages_cat = list(categorymembers.values()) #toutes les pages de la catégorie\n",
    "\n",
    "    shuffle(pages_cat)\n",
    "    print(i)\n",
    "\n",
    "    for c in pages_cat:\n",
    "        if i==1000: #1000 pages par catégorie\n",
    "            break\n",
    "\n",
    "        if c.title[:6]!=\"Portal\" and c.title[:5]!=\"File:\" and c.title[:8]!=\"Category\": # vérifie que c'est bien un article\n",
    "            i+=1\n",
    "            #list_urls.append(c.fullurl)\n",
    "            #list_pages.append(c)\n",
    "            #list_pages.append(c.title)\n",
    "            d = {}\n",
    "            d[\"titre\"] = c.title\n",
    "            d[\"contenu\"] = c.text\n",
    "            d[\"liens\"] = list(c.links.keys())\n",
    "            list_pages.append(d)\n",
    "\n",
    "        if c.ns == wikipediaapi.Namespace.CATEGORY and level < max_level: # descend d'un niveau\n",
    "            i = url_categorymembers(c.categorymembers, list_pages, level=level + 1, max_level=max_level, i=i)[1]\n",
    "\n",
    "    return list_pages, i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_arts = wiki_wiki.page(\"Category:The arts\") # Arts\n",
    "arts, _ = url_categorymembers(cat_arts.categorymembers)\n",
    "pd.DataFrame(arts).to_csv('arts.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_games = wiki_wiki.page(\"Category:Games\") # Games\n",
    "games, _= url_categorymembers(cat_games.categorymembers)\n",
    "\n",
    "pd.DataFrame(games).to_csv('games.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_youth = wiki_wiki.page(\"Category:Youth\") # Kids and Teens (not exact)\n",
    "youth, _ = url_categorymembers(cat_youth.categorymembers)\n",
    "\n",
    "pd.DataFrame(youth).to_csv('youth.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_reference = wiki_wiki.page(\"Category:Reference\") # Reference\n",
    "reference, _ = url_categorymembers(cat_reference.categorymembers)\n",
    "\n",
    "pd.DataFrame(reference).to_csv('reference.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_shopping = wiki_wiki.page(\"Category:Retailing\") # Shopping\n",
    "shopping, _ = url_categorymembers(cat_shopping.categorymembers)\n",
    "\n",
    "pd.DataFrame(shopping).to_csv('shopping.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_business = wiki_wiki.page(\"Category:Business\") # Business\n",
    "business, _ = url_categorymembers(cat_business.categorymembers)\n",
    "\n",
    "pd.DataFrame(business).to_csv('business.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_health = wiki_wiki.page(\"Category:Health\") # Health\n",
    "health, _ = url_categorymembers(cat_health.categorymembers)\n",
    "\n",
    "pd.DataFrame(health).to_csv('health.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_news = wiki_wiki.page(\"Category:News\") # News\n",
    "news, _ = url_categorymembers(cat_news.categorymembers)\n",
    "\n",
    "pd.DataFrame(news).to_csv('news.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_geography = wiki_wiki.page(\"Category:Geography\") # Regional (not exact)\n",
    "geography, _ = url_categorymembers(cat_geography.categorymembers)\n",
    "\n",
    "pd.DataFrame(geography).to_csv('geography.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_society = wiki_wiki.page(\"Category:Society\") # Society\n",
    "society, _ = url_categorymembers(cat_society.categorymembers)\n",
    "\n",
    "pd.DataFrame(society).to_csv('society.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_computers = wiki_wiki.page(\"Category:Computers\") # Computers\n",
    "computers, _ = url_categorymembers(cat_computers.categorymembers)\n",
    "\n",
    "df = pd.DataFrame(computers)\n",
    "df.to_csv('computers.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_home = wiki_wiki.page(\"Category:Home\") # Home\n",
    "home, _ = url_categorymembers(cat_home.categorymembers)\n",
    "\n",
    "df = pd.DataFrame(home)\n",
    "df.to_csv('home.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_recreation = wiki_wiki.page(\"Category:Recreation\") # Recreation\n",
    "recreation, _ = url_categorymembers(cat_recreation.categorymembers)\n",
    "\n",
    "pd.DataFrame(recreation).to_csv('recreation.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_science = wiki_wiki.page(\"Category:Science\") # Science\n",
    "science, _ = url_categorymembers(cat_science.categorymembers)\n",
    "\n",
    "pd.DataFrame(science).to_csv('science.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_sports = wiki_wiki.page(\"Category:Sports\") # Sports\n",
    "sports, _ = url_categorymembers(cat_sports.categorymembers)\n",
    "\n",
    "pd.DataFrame(sports).to_csv('sports.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_world = wiki_wiki.page(\"Category:World\") # World\n",
    "world, _ = url_categorymembers(cat_world.categorymembers)\n",
    "\n",
    "pd.DataFrame(world).to_csv('world.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pas exécuté\n",
    "\n",
    "topics = {\n",
    "    \"Arts\": arts,\n",
    "    \"Games\": games,\n",
    "    \"Youth\": youth,\n",
    "    \"Reference\": reference, \n",
    "    \"Shopping\": shopping, \n",
    "    \"Business\": business, \n",
    "    \"Health\": health, \n",
    "    \"News\": news,\n",
    "    \"Geography\": geography,\n",
    "    \"Society\": society,\n",
    "    \"Computers\": computers,\n",
    "    \"Home\": home,\n",
    "    \"Recreation\": recreation,\n",
    "    \"Science\": science,\n",
    "    \"Sports\": sports,\n",
    "    \"World\": world\n",
    "}\n",
    "\n",
    "\n",
    "import pickle \n",
    "\n",
    "with open('titres_articles_categories.pkl', 'wb') as f:\n",
    "    pickle.dump(topics, f)\n",
    "        \n",
    "#with open('titres_articles_categories.pkl', 'rb') as f:\n",
    "    #topics = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "csv_files = [\n",
    "    'arts.csv', 'business.csv', 'computers.csv', 'games.csv',\n",
    "    'geography.csv', 'health.csv', 'home.csv', 'news.csv',\n",
    "    'recreation.csv', 'reference.csv', 'science.csv', 'shopping.csv',\n",
    "    'society.csv', 'sports.csv', 'world.csv', 'youth.csv'\n",
    "]\n",
    "\n",
    "# Liste pour stocker les DataFrames\n",
    "dataframes = []\n",
    "\n",
    "# Lire chaque fichier CSV et l'ajouter à la liste des DataFrames\n",
    "for file in csv_files:\n",
    "    df = pd.read_csv(f'./Data/{file}')\n",
    "    dataframes.append(df)\n",
    "\n",
    "# Combiner tous les DataFrames en un seul\n",
    "combined_df = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "# Écrire le DataFrame combiné dans un nouveau fichier CSV\n",
    "combined_df.to_csv('./Data/all.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Indexation des articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyterrier as pt\n",
    "if not pt.started():\n",
    "    pt.init()\n",
    "\n",
    "df = pd.DataFrame(liste_documents, columns=[\"docno\", \"text\"])\n",
    "\n",
    "index_path = \"./index\"\n",
    "\n",
    "indexer = pt.IterDictIndexer(index_path)\n",
    "index_ref = indexer.index(df[\"text\"], df[\"docno\"])\n",
    "\n",
    "index = pt.IndexFactory.of(index_ref)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute Page Rank vectors (3.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrices d'adjacence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construction de la matrice sans utiliser les fichiers :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "# build the adjacency matrix\n",
    "def build_adjacency_matrix(df):\n",
    "    import ast\n",
    "    num_pages = df.shape[0]\n",
    "    adjacency_matrix = np.zeros((num_pages, num_pages))\n",
    "    df[\"liens\"] = df[\"liens\"].apply(ast.literal_eval)\n",
    "    \n",
    "    for index, row in df.iterrows():\n",
    "        for link in row['liens']:\n",
    "            if link in df['titre'].values:\n",
    "                j = np.where(df['titre'] == link )[0]\n",
    "                adjacency_matrix[index][j] += 1\n",
    "\n",
    "    return adjacency_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrice_arts = build_adjacency_matrix(pd.read_csv('./Data/arts.csv'))\n",
    "np.save('./Matrices/matrice_arts.npy', matrice_arts)\n",
    "\n",
    "matrice_games = build_adjacency_matrix(pd.read_csv('./Data/games.csv'))\n",
    "np.save('./Matrices/matrice_games.npy', matrice_games)\n",
    "\n",
    "matrice_youth = build_adjacency_matrix(pd.read_csv('./Data/youth.csv'))\n",
    "np.save('./Matrices/matrice_youth.npy', matrice_youth)\n",
    "\n",
    "matrice_reference = build_adjacency_matrix(pd.read_csv('./Data/reference.csv'))\n",
    "np.save('./Matrices/matrice_reference.npy', matrice_reference)\n",
    "\n",
    "matrice_shopping = build_adjacency_matrix(pd.read_csv('./Data/shopping.csv'))\n",
    "np.save('./Matrices/matrice_shopping.npy', matrice_shopping)\n",
    "\n",
    "matrice_business = build_adjacency_matrix(pd.read_csv('./Data/business.csv'))\n",
    "np.save('./Matrices/matrice_business.npy', matrice_business)\n",
    "\n",
    "matrice_health = build_adjacency_matrix(pd.read_csv('./Data/health.csv'))\n",
    "np.save('./Matrices/matrice_health.npy', matrice_health)\n",
    "\n",
    "matrice_news = build_adjacency_matrix(pd.read_csv('./Data/news.csv'))\n",
    "np.save('./Matrices/matrice_news.npy', matrice_news)\n",
    "\n",
    "matrice_geography = build_adjacency_matrix(pd.read_csv('./Data/geography.csv'))\n",
    "np.save('./Matrices/matrice_geography.npy', matrice_geography)\n",
    "\n",
    "matrice_society = build_adjacency_matrix(pd.read_csv('./Data/society.csv'))\n",
    "np.save('./Matrices/matrice_society.npy', matrice_society)\n",
    "\n",
    "matrice_computers = build_adjacency_matrix(pd.read_csv('./Data/computers.csv'))\n",
    "np.save('./Matrices/matrice_computers.npy', matrice_computers)\n",
    "\n",
    "matrice_home = build_adjacency_matrix(pd.read_csv('./Data/home.csv'))\n",
    "np.save('./Matrices/matrice_home.npy', matrice_home)\n",
    "\n",
    "matrice_recreation = build_adjacency_matrix(pd.read_csv('./Data/recreation.csv'))\n",
    "np.save('./Matrices/matrice_recreation.npy', matrice_recreation)\n",
    "\n",
    "matrice_science = build_adjacency_matrix(pd.read_csv('./Data/science.csv'))\n",
    "np.save('./Matrices/matrice_science.npy', matrice_science)\n",
    "\n",
    "matrice_sports = build_adjacency_matrix(pd.read_csv('./Data/sports.csv'))\n",
    "np.save('./Matrices/matrice_sports.npy', matrice_sports)\n",
    "\n",
    "matrice_world = build_adjacency_matrix(pd.read_csv('./Data/world.csv'))\n",
    "np.save('./Matrices/matrice_world.npy', matrice_world)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrice_all= build_adjacency_matrix(pd.read_csv('./Data/all.csv'))\n",
    "np.save('./Matrices/matrice_all.npy', matrice_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construction de la matrice en utilisant les fichiers :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "fichier_graphe = pandas.read_csv('enwiki.wikilink_graph.2018-03-01.csv')\n",
    "\n",
    "# build the adjacency matrix\n",
    "def build_adjacency_matrix(topic_articles, inversed_index_articles):\n",
    "    num_pages = len(topic_articles)\n",
    "    adjacency_matrix = np.zeros((num_pages, num_pages))\n",
    "\n",
    "    for i,titre in enumerate(topic_articles):\n",
    "        #links = page_links(pages[i])\n",
    "        links = fichier_graphe[fichier_graphe['page_title_from']==titre]\n",
    "        for link in links:\n",
    "            if link in topic_articles:\n",
    "                j = topic_articles.index(link)\n",
    "                adjacency_matrix[i][j] = 1\n",
    "\n",
    "    return adjacency_matrix/sum(adjacency_matrix)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calcul des vecteurs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute PageRank scores\n",
    "def compute_pagerank(M, alpha=0.25, bias = None): # bias = None pour page_rank normal, bias = topic sinon\n",
    "    # M = adjacency_matrix\n",
    "    # damping_factor = 1-alpha\n",
    "\n",
    "    N = M.shape[0]\n",
    "\n",
    "    if bias is None : # page_rank normal, pas topic_sensitive\n",
    "        p = np.ones((N, 1)) / N \n",
    "\n",
    "    else: # ODP-biasing \n",
    "        articles = pd.read_csv(f'./Data/all.csv')[\"titre\"].values\n",
    "        articles_cat = pd.read_csv(f'./Data/{bias}.csv')[\"titre\"].values\n",
    "        p = np.where(np.isin(articles,articles_cat), 1/len(articles_cat), 0)\n",
    "\n",
    "    #M_prime = (1-alpha) * M + alpha * teleportation_matrix\n",
    "\n",
    "    rank = np.ones(N) / N\n",
    "    old_rank = np.zeros(N)\n",
    "\n",
    "    epsilon = 1.0e-3\n",
    "    max_iterations = 100\n",
    "    iterations = 0\n",
    "\n",
    "    while np.sum(np.abs(rank - old_rank)) > epsilon and iterations < max_iterations:\n",
    "        old_rank = rank.copy()\n",
    "        rank = (1-alpha)*M @ rank + p # equation 5\n",
    "        iterations += 1\n",
    "\n",
    "    return rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: arts\n",
      "[1.29477208e+199 3.66818058e+197 3.18144580e+199 1.94832834e+198\n",
      " 3.27135694e+200]\n",
      "Topic: games\n",
      "[1.29476648e+199 3.66816473e+197 3.18143204e+199 1.94831992e+198\n",
      " 3.27134281e+200]\n",
      "Topic: youth\n",
      "[1.29480276e+199 3.66826749e+197 3.18152117e+199 1.94837450e+198\n",
      " 3.27143445e+200]\n",
      "Topic: reference\n",
      "[1.29485319e+199 3.66841038e+197 3.18164510e+199 1.94845039e+198\n",
      " 3.27156188e+200]\n",
      "Topic: shopping\n",
      "[1.29477174e+199 3.66817961e+197 3.18144495e+199 1.94832782e+198\n",
      " 3.27135608e+200]\n",
      "Topic: business\n",
      "[1.29477565e+199 3.66819070e+197 3.18145457e+199 1.94833371e+198\n",
      " 3.27136596e+200]\n",
      "Topic: health\n",
      "[1.29477143e+199 3.66817875e+197 3.18144421e+199 1.94832737e+198\n",
      " 3.27135531e+200]\n",
      "Topic: news\n",
      "[1.29484185e+199 3.66837826e+197 3.18161724e+199 1.94843333e+198\n",
      " 3.27153323e+200]\n",
      "Topic: geography\n",
      "[1.45251324e+199 4.11507242e+197 3.56903908e+199 2.18569180e+198\n",
      " 3.66990404e+200]\n",
      "Topic: society\n",
      "[1.29481087e+199 3.66829048e+197 3.18154111e+199 1.94838671e+198\n",
      " 3.27145495e+200]\n",
      "Topic: computers\n",
      "[1.29476659e+199 3.66816502e+197 3.18143230e+199 1.94832007e+198\n",
      " 3.27134307e+200]\n",
      "Topic: home\n",
      "[1.29477894e+199 3.66820003e+197 3.18146266e+199 1.94833867e+198\n",
      " 3.27137429e+200]\n",
      "Topic: recreation\n",
      "[1.29477675e+199 3.66819382e+197 3.18145727e+199 1.94833537e+198\n",
      " 3.27136875e+200]\n",
      "Topic: science\n",
      "[1.29477228e+199 3.66818116e+197 3.18144630e+199 1.94832865e+198\n",
      " 3.27135746e+200]\n",
      "Topic: sports\n",
      "[1.29478195e+199 3.66820856e+197 3.18147006e+199 1.94834320e+198\n",
      " 3.27138189e+200]\n",
      "Topic: world\n",
      "[1.29479044e+199 3.66823259e+197 3.18149090e+199 1.94835596e+198\n",
      " 3.27140332e+200]\n"
     ]
    }
   ],
   "source": [
    "# Compute PageRank scores for each topic\n",
    "\n",
    "Topic_names = [\"arts\",\"games\",\"youth\",\"reference\", \"shopping\", \"business\", \"health\", \"news\",\"geography\",\"society\",\"computers\",\"home\",\"recreation\",\"science\",\"sports\",\"world\"]\n",
    "\n",
    "topic_pagerank = {}\n",
    "for topic in Topic_names:\n",
    "    #adjacency_matrix = np.load(f'./Matrices/matrice_{topic}.npy')\n",
    "    adjacency_matrix = np.load(f'./Matrices/matrice_all.npy')\n",
    "    pagerank_scores = compute_pagerank(adjacency_matrix,bias=topic)\n",
    "\n",
    "    print(f\"Topic: {topic}\")\n",
    "    print(pagerank_scores[:5])\n",
    "\n",
    "    topic_pagerank[topic] = pagerank_scores\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('topic_pagerank.pkl', 'wb') as fichier:\n",
    "    pickle.dump(topic_pagerank, fichier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Similarity ranking (4.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def OSim(t1, t2, n=20):\n",
    "    # t1, t2 listes\n",
    "    return len(set(t1[:n])&set(t2[:n]))/n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "def KSim(t1, t2):\n",
    "    # t1, t2 listes\n",
    "    U = set(t1)|set(t2)\n",
    "    delta1 = U - set(t1)\n",
    "    delta2 = U - set(t2)\n",
    "    t1_prime = t1+list(delta1)\n",
    "    t2_prime = t1+list(delta2)\n",
    "    sim = 0\n",
    "    for u,v in list(itertools.permutations(U, 2)):\n",
    "        if np.sign(t1_prime.index(u)-t1_prime.index(v))==np.sign(t2_prime.index(u)-t2_prime.index(v)):\n",
    "            sim+=1\n",
    "    return sim/(len(U)*(len(U)-1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query-time importance score (3.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calcul de D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "\n",
    "categories_text = {}\n",
    "for c in Topic_names:\n",
    "    liste_textes = pd.read_csv(f'./Data/{c}.csv')[\"contenu\"].values.astype(str)\n",
    "\n",
    "    texte = ' '.join(liste_textes)\n",
    "    categories_text[c] = texte\n",
    "\n",
    "D_categories = {}\n",
    "terms_indexes_categories = {}\n",
    "\n",
    "for c, text in categories_text.items():\n",
    "    vectorizer = CountVectorizer(stop_words='english')\n",
    "    D_categories[c] = vectorizer.fit_transform([text])\n",
    "    terms_indexes_categories[c] = vectorizer.get_feature_names_out()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('D.pkl', 'wb') as fichier:\n",
    "    pickle.dump(D_categories, fichier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# à changer pour ne calculer que sur les docs qui ont les termes de la query au lieu de tout\n",
    "\n",
    "def sqd(q, topic_pagerank, D, terms_indexes):\n",
    "    # D = gives the total number of occurrences of each term in each document, array of shape = classes x terms\n",
    "    \n",
    "    p_c = 1/16 # \"The quantity P(cj) is not as straightforward. We chose to make it uniform\"\n",
    "    \n",
    "    indexes = np.in1d(terms_indexes, np.array(q)).nonzero()[0]\n",
    "\n",
    "    p_q_c = D[indexes]/D.sum(axis=1)\n",
    "\n",
    "    probas_c_q = p_c * np.product(p_q_c)\n",
    "\n",
    "    return np.sum(probas_c_q*topic_pagerank,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = [ \"affirmative action\", \"alcoholism\", \"amusement parks\", \n",
    "            \"architecture\", \"bicycling\", \"blues\", \"cheese\", \n",
    "            \"citrus groves\", \"classical guitar\", \"computer vision\", \n",
    "            \"cruises\", \"death valley\", \"field hockey\", \n",
    "            \"gardening\", \"graphic design\", \"gulf war\", \n",
    "            \"hiv\", \"java\", \"lipari\",\n",
    "            \"lyme disease\", \"mutual funds\", \"national parks\", \n",
    "            \"parallel architecture\", \"recycling cans\", \"rock climbing\", \n",
    "            \"san francisco\", \"shakespeare\", \"stamp collecting\", \n",
    "            \"sushi\", \"table tennis\", \"telecommuting\", \n",
    "            \"vintage cars\", \"volcano\", \"zen buddhism\", \"zener\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
