{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install wikipedia-api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wikipediaapi\n",
    "\n",
    "\n",
    "wiki_wiki = wikipediaapi.Wikipedia(\n",
    "    user_agent='MyProjectName',\n",
    "        language='en',\n",
    "        extract_format=wikipediaapi.ExtractFormat.WIKI\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Get data for each category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import shuffle\n",
    "\n",
    "\n",
    "def url_categorymembers(categorymembers,list_pages=[], level=0, max_level=3, i=0):\n",
    "    pages_cat = list(categorymembers.values()) #toutes les pages de la catégorie\n",
    "    shuffle(pages_cat)\n",
    "\n",
    "    for c in pages_cat:\n",
    "        if i==1000: #1000 pages par catégorie\n",
    "            break\n",
    "\n",
    "        if c.title[:6]!=\"Portal\" and c.title[:5]!=\"File:\" and c.title[:8]!=\"Category\": # vérifie que c'est bien un article\n",
    "            i+=1\n",
    "            d = {}\n",
    "            d[\"titre\"] = c.title\n",
    "            d[\"contenu\"] = c.text\n",
    "            d[\"liens\"] = list(c.links.keys())\n",
    "            list_pages.append(d)\n",
    "\n",
    "        if c.ns == wikipediaapi.Namespace.CATEGORY and level < max_level: # descend d'un niveau\n",
    "            i = url_categorymembers(c.categorymembers, list_pages, level=level + 1, max_level=max_level, i=i)[1]\n",
    "\n",
    "    return list_pages, i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_arts = wiki_wiki.page(\"Category:The arts\") # Arts\n",
    "arts, _ = url_categorymembers(cat_arts.categorymembers)\n",
    "pd.DataFrame(arts).to_csv('arts.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_games = wiki_wiki.page(\"Category:Games\") # Games\n",
    "games, _= url_categorymembers(cat_games.categorymembers)\n",
    "\n",
    "pd.DataFrame(games).to_csv('games.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_youth = wiki_wiki.page(\"Category:Youth\") # Kids and Teens (not exact)\n",
    "youth, _ = url_categorymembers(cat_youth.categorymembers)\n",
    "\n",
    "pd.DataFrame(youth).to_csv('youth.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_reference = wiki_wiki.page(\"Category:Reference\") # Reference\n",
    "reference, _ = url_categorymembers(cat_reference.categorymembers)\n",
    "\n",
    "pd.DataFrame(reference).to_csv('reference.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_shopping = wiki_wiki.page(\"Category:Retailing\") # Shopping\n",
    "shopping, _ = url_categorymembers(cat_shopping.categorymembers)\n",
    "\n",
    "pd.DataFrame(shopping).to_csv('shopping.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_business = wiki_wiki.page(\"Category:Business\") # Business\n",
    "business, _ = url_categorymembers(cat_business.categorymembers)\n",
    "\n",
    "pd.DataFrame(business).to_csv('business.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_health = wiki_wiki.page(\"Category:Health\") # Health\n",
    "health, _ = url_categorymembers(cat_health.categorymembers)\n",
    "\n",
    "pd.DataFrame(health).to_csv('health.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_news = wiki_wiki.page(\"Category:News\") # News\n",
    "news, _ = url_categorymembers(cat_news.categorymembers)\n",
    "\n",
    "pd.DataFrame(news).to_csv('news.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_geography = wiki_wiki.page(\"Category:Geography\") # Regional (not exact)\n",
    "geography, _ = url_categorymembers(cat_geography.categorymembers)\n",
    "\n",
    "pd.DataFrame(geography).to_csv('geography.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_society = wiki_wiki.page(\"Category:Society\") # Society\n",
    "society, _ = url_categorymembers(cat_society.categorymembers)\n",
    "\n",
    "pd.DataFrame(society).to_csv('society.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_computers = wiki_wiki.page(\"Category:Computers\") # Computers\n",
    "computers, _ = url_categorymembers(cat_computers.categorymembers)\n",
    "\n",
    "pd.DataFrame(computers).to_csv('computers.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_home = wiki_wiki.page(\"Category:Home\") # Home\n",
    "home, _ = url_categorymembers(cat_home.categorymembers)\n",
    "\n",
    "pd.DataFrame(home).to_csv('home.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_recreation = wiki_wiki.page(\"Category:Recreation\") # Recreation\n",
    "recreation, _ = url_categorymembers(cat_recreation.categorymembers)\n",
    "\n",
    "pd.DataFrame(recreation).to_csv('recreation.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_science = wiki_wiki.page(\"Category:Science\") # Science\n",
    "science, _ = url_categorymembers(cat_science.categorymembers)\n",
    "\n",
    "pd.DataFrame(science).to_csv('science.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_sports = wiki_wiki.page(\"Category:Sports\") # Sports\n",
    "sports, _ = url_categorymembers(cat_sports.categorymembers)\n",
    "\n",
    "pd.DataFrame(sports).to_csv('sports.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_world = wiki_wiki.page(\"Category:World\") # World\n",
    "world, _ = url_categorymembers(cat_world.categorymembers)\n",
    "\n",
    "pd.DataFrame(world).to_csv('world.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_files = [\n",
    "    'arts.csv', 'business.csv', 'computers.csv', 'games.csv',\n",
    "    'geography.csv', 'health.csv', 'home.csv', 'news.csv',\n",
    "    'recreation.csv', 'reference.csv', 'science.csv', 'shopping.csv',\n",
    "    'society.csv', 'sports.csv', 'world.csv', 'youth.csv'\n",
    "]\n",
    "\n",
    "# Liste pour stocker les DataFrames\n",
    "dataframes = []\n",
    "\n",
    "# Lire chaque fichier CSV et l'ajouter à la liste des DataFrames\n",
    "for file in csv_files:\n",
    "    df = pd.read_csv(f'./Data/{file}')\n",
    "    dataframes.append(df)\n",
    "\n",
    "# Combiner tous les DataFrames en un seul\n",
    "combined_df = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "# Écrire le DataFrame combiné dans un nouveau fichier CSV\n",
    "combined_df.to_csv('./Data/all.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute Page Rank vectors (3.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrices d'adjacence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construction de la matrice sans utiliser les fichiers :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the adjacency matrix\n",
    "def build_adjacency_matrix_2(df): #pas optimisé\n",
    "    num_pages = df.shape[0]\n",
    "    adjacency_matrix = np.zeros((num_pages, num_pages))\n",
    "    df[\"liens\"] = df[\"liens\"].apply(ast.literal_eval)\n",
    "    \n",
    "    for index, row in df.iterrows():\n",
    "        N = len(np.where(np.isin(row['liens'],df['titre'].values))[0]) # nombres de liens que fait la page\n",
    "        for link in row['liens']:\n",
    "            if link in df['titre'].values:\n",
    "                j = np.where(df['titre'] == link )[0]\n",
    "                adjacency_matrix[j,index] += 1/N\n",
    "\n",
    "    return adjacency_matrix\n",
    "\n",
    "\n",
    "\n",
    "def build_adjacency_matrix(df):\n",
    "    df[\"liens\"] = df[\"liens\"].apply(ast.literal_eval)  # Convertir les liens de type string en listes\n",
    "\n",
    "    # Créer un dictionnaire pour mapper les titres aux indices\n",
    "    title_to_index = {title: i for i, title in enumerate(df['titre'])}\n",
    "    \n",
    "    # Initialiser la matrice d'adjacence avec des zéros\n",
    "    num_pages = len(df)\n",
    "    adjacency_matrix = np.zeros((num_pages, num_pages))\n",
    "    \n",
    "    # Construire une série pour mapper les liens aux indices\n",
    "    link_series = pd.Series(df['liens'].values.tolist())\n",
    "    link_indices = link_series.map(lambda x: [title_to_index[link] for link in x if link in title_to_index])\n",
    "    \n",
    "    # Compter le nombre de liens sortants pour chaque article\n",
    "    df['num_liens'] = link_indices.apply(len)\n",
    "    \n",
    "    for i, indices in enumerate(link_indices):\n",
    "        if len(indices)>0:\n",
    "            adjacency_matrix[indices, i] = 1 / df.at[i, 'num_liens']\n",
    "    \n",
    "    return adjacency_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrice_all= build_adjacency_matrix(pd.read_csv('./Data/all.csv'))\n",
    "np.save('./Matrices/matrice_all.npy', matrice_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construction de la matrice en utilisant les fichiers :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fichier_graphe = pd.read_csv('enwiki.wikilink_graph.2018-03-01.csv')\n",
    "\n",
    "# build the adjacency matrix\n",
    "def build_adjacency_matrix(topic_articles, inversed_index_articles):\n",
    "    num_pages = len(topic_articles)\n",
    "    adjacency_matrix = np.zeros((num_pages, num_pages))\n",
    "\n",
    "    for i,titre in enumerate(topic_articles):\n",
    "        #links = page_links(pages[i])\n",
    "        links = fichier_graphe[fichier_graphe['page_title_from']==titre]\n",
    "        for link in links:\n",
    "            if link in topic_articles:\n",
    "                j = topic_articles.index(link)\n",
    "                adjacency_matrix[j][i] = 1\n",
    "\n",
    "    return adjacency_matrix/sum(adjacency_matrix,axis=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calcul des vecteurs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute PageRank scores\n",
    "def compute_pagerank(M, alpha=0.25, bias = None): # bias = None pour page_rank normal, bias = topic sinon\n",
    "    # M = adjacency_matrix\n",
    "    # damping_factor = 1-alpha\n",
    "\n",
    "    N = M.shape[0]\n",
    "\n",
    "    if bias is None : # page_rank normal, pas topic_sensitive\n",
    "        p = np.ones(N) / N \n",
    "\n",
    "    else: # ODP-biasing \n",
    "        articles = pd.read_csv(f'./Data/all.csv')[\"titre\"].values\n",
    "        articles_cat = pd.read_csv(f'./Data/{bias}.csv')[\"titre\"].values\n",
    "        p = np.where(np.isin(articles,articles_cat), 1/len(articles_cat), 0)\n",
    "\n",
    "    #M_prime = (1-alpha) * M + alpha * teleportation_matrix\n",
    "\n",
    "    rank = np.ones(N) / N\n",
    "    old_rank = np.zeros(N)\n",
    "\n",
    "    epsilon = 1.0e-3\n",
    "    max_iterations = 10\n",
    "    iterations = 0\n",
    "\n",
    "    while np.sum(np.abs(rank - old_rank)) > epsilon and iterations < max_iterations:\n",
    "        old_rank = rank.copy()\n",
    "        rank = (1-alpha)*M @ rank + p # equation 5\n",
    "        #normaliser ?\n",
    "        iterations += 1\n",
    "\n",
    "    return rank"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T23:51:51.476135Z",
     "start_time": "2024-05-21T23:51:51.421210Z"
    }
   },
   "source": [
    "# Compute PageRank scores for each topic\n",
    "import numpy as np\n",
    "\n",
    "Topic_names = [\"arts\",\"games\",\"youth\",\"reference\", \"shopping\", \"business\", \"health\", \"news\",\"geography\",\"society\",\"computers\",\"home\",\"recreation\",\"science\",\"sports\",\"world\"]\n",
    "\n",
    "adjacency_matrix = np.load(f'./Matrices/matrice_all.npy')\n",
    "\n",
    "topic_pagerank = {}\n",
    "for topic in Topic_names:\n",
    "    #adjacency_matrix = np.load(f'./Matrices/matrice_{topic}.npy')\n",
    "    \n",
    "    pagerank_scores = compute_pagerank(adjacency_matrix,bias=topic)\n",
    "\n",
    "    print(f\"Topic: {topic}\")\n",
    "    print(pagerank_scores[:5])\n",
    "\n",
    "    topic_pagerank[topic] = pagerank_scores\n",
    "\n",
    "    "
   ],
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './Matrices/matrice_all.npy'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[2], line 6\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mnumpy\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mnp\u001B[39;00m\n\u001B[1;32m      4\u001B[0m Topic_names \u001B[38;5;241m=\u001B[39m [\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124marts\u001B[39m\u001B[38;5;124m\"\u001B[39m,\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mgames\u001B[39m\u001B[38;5;124m\"\u001B[39m,\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124myouth\u001B[39m\u001B[38;5;124m\"\u001B[39m,\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mreference\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mshopping\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbusiness\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhealth\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnews\u001B[39m\u001B[38;5;124m\"\u001B[39m,\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mgeography\u001B[39m\u001B[38;5;124m\"\u001B[39m,\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msociety\u001B[39m\u001B[38;5;124m\"\u001B[39m,\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcomputers\u001B[39m\u001B[38;5;124m\"\u001B[39m,\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhome\u001B[39m\u001B[38;5;124m\"\u001B[39m,\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrecreation\u001B[39m\u001B[38;5;124m\"\u001B[39m,\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mscience\u001B[39m\u001B[38;5;124m\"\u001B[39m,\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msports\u001B[39m\u001B[38;5;124m\"\u001B[39m,\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mworld\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[0;32m----> 6\u001B[0m adjacency_matrix \u001B[38;5;241m=\u001B[39m \u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43mf\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43m./Matrices/matrice_all.npy\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m      8\u001B[0m topic_pagerank \u001B[38;5;241m=\u001B[39m {}\n\u001B[1;32m      9\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m topic \u001B[38;5;129;01min\u001B[39;00m Topic_names:\n\u001B[1;32m     10\u001B[0m     \u001B[38;5;66;03m#adjacency_matrix = np.load(f'./Matrices/matrice_{topic}.npy')\u001B[39;00m\n",
      "File \u001B[0;32m/opt/homebrew/lib/python3.11/site-packages/numpy/lib/npyio.py:427\u001B[0m, in \u001B[0;36mload\u001B[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001B[0m\n\u001B[1;32m    425\u001B[0m     own_fid \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[1;32m    426\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 427\u001B[0m     fid \u001B[38;5;241m=\u001B[39m stack\u001B[38;5;241m.\u001B[39menter_context(\u001B[38;5;28;43mopen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mos_fspath\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfile\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mrb\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m)\n\u001B[1;32m    428\u001B[0m     own_fid \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m    430\u001B[0m \u001B[38;5;66;03m# Code to distinguish from NumPy binary files and pickles.\u001B[39;00m\n",
      "\u001B[0;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: './Matrices/matrice_all.npy'"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('topic_pagerank.pkl', 'wb') as fichier:\n",
    "    pickle.dump(topic_pagerank, fichier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adjacency_matrix = np.load(f'./Matrices/matrice_all.npy')\n",
    "pagerank_normal = compute_pagerank(adjacency_matrix,bias=None)\n",
    "with open('normal_pagerank.pkl', 'wb') as fichier:\n",
    "    pickle.dump(pagerank_normal, fichier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query-time importance score (3.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calcul de D"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T23:51:29.112892Z",
     "start_time": "2024-05-21T23:51:26.709837Z"
    }
   },
   "source": [
    "import json\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "\n",
    "categories_text = {}\n",
    "for c in Topic_names:\n",
    "    liste_textes = pd.read_csv(f'./Data/{c}.csv')[\"contenu\"].values.astype(str)\n",
    "\n",
    "    texte = ' '.join(liste_textes)\n",
    "    categories_text[c] = texte\n",
    "\n",
    "D_categories = {}\n",
    "terms_indexes_categories = {}\n",
    "\n",
    "for c, text in categories_text.items():\n",
    "    vectorizer = CountVectorizer(stop_words='english')\n",
    "    D_categories[c] = vectorizer.fit_transform([text])\n",
    "    terms_indexes_categories[c] = vectorizer.get_feature_names_out()\n",
    "    "
   ],
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Topic_names' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[1], line 6\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msklearn\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mfeature_extraction\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mtext\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m CountVectorizer\n\u001B[1;32m      5\u001B[0m categories_text \u001B[38;5;241m=\u001B[39m {}\n\u001B[0;32m----> 6\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m c \u001B[38;5;129;01min\u001B[39;00m \u001B[43mTopic_names\u001B[49m:\n\u001B[1;32m      7\u001B[0m     liste_textes \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mread_csv(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m./Data/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mc\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.csv\u001B[39m\u001B[38;5;124m'\u001B[39m)[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcontenu\u001B[39m\u001B[38;5;124m\"\u001B[39m]\u001B[38;5;241m.\u001B[39mvalues\u001B[38;5;241m.\u001B[39mastype(\u001B[38;5;28mstr\u001B[39m)\n\u001B[1;32m      9\u001B[0m     texte \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mjoin(liste_textes)\n",
      "\u001B[0;31mNameError\u001B[0m: name 'Topic_names' is not defined"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T22:54:48.510738Z",
     "start_time": "2024-05-21T22:54:48.510677Z"
    }
   },
   "source": [
    "with open('D.pkl', 'wb') as fichier:\n",
    "    pickle.dump(D_categories, fichier)\n",
    "\n",
    "with open('terms_indexes.pkl', 'wb') as fichier:\n",
    "    pickle.dump(terms_indexes_categories, fichier)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install python-terrier"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T23:52:13.918766Z",
     "start_time": "2024-05-21T23:52:04.178459Z"
    }
   },
   "source": [
    "# Indexation des articles \n",
    "import pandas as pd\n",
    "import pyterrier as pt\n",
    "\n",
    "if not pt.started():\n",
    "  pt.init(boot_packages=[\"com.github.terrierteam:terrier-prf:-SNAPSHOT\"])\n",
    "\n",
    "df = pd.read_csv(f'./Data/all.csv')[[\"titre\",\"contenu\"]]\n",
    "df = df.dropna()\n",
    "df.columns = [\"docno\", \"text\"]\n",
    "\n",
    "# pd_indexer = pt.DFIndexer(\"./pd_index\")\n",
    "# indexref = pd_indexer.index(df[\"text\"], df[\"docno\"])\n",
    "# index = pt.IndexFactory.of(indexref)\n",
    "\n",
    "Topic_names = [\"arts\",\"games\",\"youth\",\"reference\", \"shopping\", \"business\", \"health\", \"news\",\"geography\",\"society\",\"computers\",\"home\",\"recreation\",\"science\",\"sports\",\"world\"]"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTerrier 0.10.0 has loaded Terrier 5.9 (built by craigm on 2024-05-02 17:40) and terrier-helper 0.0.8\n",
      "\n",
      "No etc/terrier.properties, using terrier.default.properties for bootstrap configuration.\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T23:52:18.048102Z",
     "start_time": "2024-05-21T23:52:16.619079Z"
    }
   },
   "source": [
    "df = pd.read_csv(f'./Data/all.csv')[[\"titre\",\"contenu\"]]\n",
    "df = df.fillna('missing')"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T23:52:20.704718Z",
     "start_time": "2024-05-21T23:52:20.698455Z"
    }
   },
   "source": [
    "def sqd(q, topic_pagerank, D, terms_indexes, index):\n",
    "    # Using a text index, recuperer les docs qui contiennent q\n",
    "    br = pt.BatchRetrieve(index, wmodel=\"CoordinateMatch\") # CoordinateMatch renvoie 1 si le terme est dans le doc, 0 sinon\n",
    "    docs_retrieved = br.search(q) # Renvoie le nombre de termes de la query qui sont dans chaque doc\n",
    "    docs_query = docs_retrieved[docs_retrieved[\"score\"] == len(q.split())]  # Garder uniquement les docs qui contiennent tous les termes de la query\n",
    "    docs_index = docs_query[\"docno\"].values\n",
    "    \n",
    "    # calculer P(cj|q)\n",
    "    probas_c_q = np.zeros((16))\n",
    "    p_c = 1/16 # \"The quantity P(cj) is not as straightforward. We chose to make it uniform\"\n",
    "\n",
    "    for i,topic in enumerate(Topic_names):\n",
    "        indexes = np.in1d(terms_indexes[topic], np.array(q)).nonzero()[0]\n",
    "\n",
    "        if len(indexes)==0:\n",
    "            probas_c_q[i] = 0\n",
    "        else:\n",
    "            p_q_c = D[topic].tocsc()[indexes]/D[topic].tocsc().sum()\n",
    "            p_q_c = p_q_c.toarray()\n",
    "            probas_c_q[i] = p_c * np.prod(p_q_c)\n",
    "            \n",
    "    df = pd.read_csv(f'./Data/all.csv')\n",
    "    df.columns = [\"id\", \"titre\", \"contenu\", \"liens\"]\n",
    "            \n",
    "    docs = df[df[\"titre\"].isin(docs_index)].copy()\n",
    "    \n",
    "    # print(docs)\n",
    "    \n",
    "    docs[\"score\"] = docs.apply(lambda x: sum([probas_c_q[Topic_names.index(topic)]*topic_pagerank[topic][x[\"id\"]] for topic in Topic_names]), axis=1)\n",
    "    \n",
    "    # print(docs)\n",
    "\n",
    "    # garder les 3 meilleurs cj\n",
    "    # sorted_indices = np.argsort(-probas_c_q)\n",
    "    # \n",
    "    # top_indices = sorted_indices[:3]\n",
    "    # top_values = probas_c_q[top_indices]\n",
    "    # \n",
    "    # # somme des P(cj|q) * rank j\n",
    "    # res = 0\n",
    "    # for i in range(3):\n",
    "    #     res+= top_values[i]*topic_pagerank[Topic_names[top_indices[i]]]\n",
    "    #return np.sum(top_values*topic_pagerank[top_indices],axis=1)\n",
    "\n",
    "\n",
    "    # meilleurs documents :\n",
    "\n",
    "    # doc qui ont les mots de la query sans pyterrier\n",
    "    # df['text_lower'] = df['contenu'].str.lower() # Convert the text column to lowercase for case-insensitive search\n",
    "    # query_words = q.lower().split() # Split the query into individual words\n",
    "    # mask = df['text_lower'].str.contains('|'.join(query_words)) # boolean mask to filter rows containing any of the query words\n",
    "    # doc_indices = df.index[mask] # indices des docs qui ont la query\n",
    "    \n",
    "    #res[doc_indices]+=100 # on leur donne plus de valeur pour qu'ils sortent d'abord\n",
    "\n",
    "    #sorted_docs_index = np.argsort(-res)\n",
    "    # best_docs = pd.read_csv(f'./Data/all.csv')[\"titre\"].values[sorted_docs_index]\n",
    "\n",
    "    best_docs = docs.sort_values(by=\"score\", ascending=False)[\"titre\"]\n",
    "    return best_docs"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T23:52:22.564578Z",
     "start_time": "2024-05-21T23:52:22.561753Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# query = \"affirmative action\"\n",
    "# print(sqd(query, topic_pagerank, D, terms_indexes, index)[:5])"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T23:52:23.097359Z",
     "start_time": "2024-05-21T23:52:23.092988Z"
    }
   },
   "source": [
    "def score_normal_pagerank(q,normal_pagerank):\n",
    "    \n",
    "    # # doc qui ont les mots de la query (sans pyterrier)\n",
    "    # df['text_lower'] = df['contenu'].str.lower() # Convert the text column to lowercase for case-insensitive search\n",
    "    # query_words = q.lower().split() # Split the query into individual words\n",
    "    # mask = df['text_lower'].str.contains('|'.join(query_words)) # boolean mask to filter rows containing any of the query words\n",
    "    # doc_indices = df.index[mask] # indices des docs qui ont la query\n",
    "    # normal_pagerank[doc_indices] +=100\n",
    "    # \n",
    "    # sorted_docs_index = np.argsort(-normal_pagerank)\n",
    "    # best_docs = pd.read_csv(f'./Data/all.csv')[\"titre\"].values[sorted_docs_index]\n",
    "    \n",
    "    # Using a text index, recuperer les docs qui contiennent q\n",
    "    br = pt.BatchRetrieve(index, wmodel=\"CoordinateMatch\") # CoordinateMatch renvoie 1 si le terme est dans le doc, 0 sinon\n",
    "    docs_retrieved = br.search(q) # Renvoie le nombre de termes de la query qui sont dans chaque doc\n",
    "    docs_query = docs_retrieved[docs_retrieved[\"score\"] == len(q.split())]  # Garder uniquement les docs qui contiennent tous les termes de la query\n",
    "    sorted_index = docs_query[\"docno\"].values\n",
    "    \n",
    "    df = pd.read_csv(f'./Data/all.csv')\n",
    "    df.columns = [\"id\", \"titre\", \"contenu\", \"liens\"]\n",
    "    \n",
    "    docs = df[df[\"titre\"].isin(sorted_index)].copy()\n",
    "    docs[\"score\"] = normal_pagerank[docs[\"id\"].values]\n",
    "    \n",
    "    best_docs = docs.sort_values(by=\"score\", ascending=False)[\"titre\"]\n",
    "    \n",
    "    return best_docs\n"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimental results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarity ranking (4.1)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T23:52:25.224579Z",
     "start_time": "2024-05-21T23:52:25.221806Z"
    }
   },
   "source": [
    "def OSim(t1, t2, n=20):\n",
    "    # t1, t2 listes\n",
    "    return len(set(t1[:n])&set(t2[:n]))/n"
   ],
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T23:52:26.147986Z",
     "start_time": "2024-05-21T23:52:26.144918Z"
    }
   },
   "source": [
    "import itertools\n",
    "\n",
    "def KSim(t1, t2):\n",
    "    # t1, t2 listes\n",
    "    U = set(t1)|set(t2)\n",
    "    delta1 = U - set(t1)\n",
    "    delta2 = U - set(t2)\n",
    "\n",
    "    t1_prime = list(t1)+list(delta1)\n",
    "    t2_prime = list(t2)+list(delta2)\n",
    "\n",
    "    sim = 0\n",
    "    for u,v in list(itertools.permutations(U, 2)):\n",
    "        if np.sign(t1_prime.index(u)-t1_prime.index(v))==np.sign(t2_prime.index(u)-t2_prime.index(v)):\n",
    "            sim+=1\n",
    "    return sim/(len(U)*(len(U)-1)) if len(U)>1 else sim\n"
   ],
   "outputs": [],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tests on queries"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T23:52:28.435815Z",
     "start_time": "2024-05-21T23:52:28.433055Z"
    }
   },
   "source": [
    "queries = [ \"affirmative action\", \"alcoholism\", \"amusement parks\", \n",
    "            \"architecture\", \"bicycling\", \"blues\", \"cheese\", \n",
    "            \"citrus groves\", \"classical guitar\", \"computer vision\", \n",
    "            \"cruises\", \"death valley\", \"field hockey\", \n",
    "            \"gardening\", \"graphic design\", \"gulf war\", \n",
    "            \"hiv\", \"java\", \"lipari\",\n",
    "            \"lyme disease\", \"mutual funds\", \"national parks\", \n",
    "            \"parallel architecture\", \"recycling cans\", \"rock climbing\", \n",
    "            \"san francisco\", \"shakespeare\", \"stamp collecting\", \n",
    "            \"sushi\", \"table tennis\", \"telecommuting\", \n",
    "            \"vintage cars\", \"volcano\", \"zen buddhism\", \"zener\"]"
   ],
   "outputs": [],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T23:52:34.415171Z",
     "start_time": "2024-05-21T23:52:34.131691Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "with open(\"terms_indexes.pkl\", \"rb\") as f:\n",
    "    terms_indexes = pickle.load(f)\n",
    "with open(\"D.pkl\", \"rb\") as f:\n",
    "    D = pickle.load(f)\n",
    "    for topic in Topic_names:\n",
    "        D[topic] = np.reshape(D[topic],(D[topic].shape[1],1))\n",
    "with open(\"topic_pagerank.pkl\", \"rb\") as f:\n",
    "    topic_pagerank = pickle.load(f)\n",
    "    \n",
    "index = pt.IndexFactory.of(\"./pd_index\")"
   ],
   "outputs": [],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T23:52:45.962795Z",
     "start_time": "2024-05-21T23:52:36.996635Z"
    }
   },
   "source": [
    "for q in queries[:5]:\n",
    "    print(q)\n",
    "    print(sqd(q, topic_pagerank, D, terms_indexes, index)[:5])\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "affirmative action\n",
      "8        Expressive therapies\n",
      "11499        Apple and unions\n",
      "12423           Shadow family\n",
      "12384           Public sphere\n",
      "12357    Gender mainstreaming\n",
      "Name: titre, dtype: object\n",
      "alcoholism\n",
      "15584           Shameless (British TV series)\n",
      "13729                     I will moida da bum\n",
      "8276                 Violence and video games\n",
      "5827                      Million Women Study\n",
      "10992    List of Stargate Universe characters\n",
      "Name: titre, dtype: object\n",
      "amusement parks\n",
      "85         Dream world (plot device)\n",
      "8879                            Park\n",
      "8762                         Soarin'\n",
      "8758    Lists of tourist attractions\n",
      "8743              Tourist attraction\n",
      "Name: titre, dtype: object\n",
      "architecture\n",
      "750                          Art\n",
      "8750                      Museum\n",
      "10584      Mapping controversies\n",
      "12584    Heritage interpretation\n",
      "5584         Imaging informatics\n",
      "Name: titre, dtype: object\n",
      "bicycling\n",
      "6273                                        Pannier\n",
      "758                                  Conceptual art\n",
      "2827                                          Intel\n",
      "14841               Global Alliance for EcoMobility\n",
      "4843     List of Colorado geographic features lists\n",
      "Name: titre, dtype: object\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T23:52:45.970098Z",
     "start_time": "2024-05-21T23:52:45.964292Z"
    }
   },
   "source": [
    "with open(\"normal_pagerank.pkl\", \"rb\") as f:\n",
    "        normal_pagerank = pickle.load(f)"
   ],
   "outputs": [],
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T23:08:55.917144Z",
     "start_time": "2024-05-21T23:08:42.377363Z"
    }
   },
   "source": [
    "for q in queries[:5]:\n",
    "    print(q)\n",
    "    print(score_normal_pagerank(q, normal_pagerank)[:5])"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "affirmative action\n",
      "14017    Universal Declaration of Human Rights\n",
      "10992     List of Stargate Universe characters\n",
      "7810                           Murray Rothbard\n",
      "14841          Global Alliance for EcoMobility\n",
      "7101                               Jactitation\n",
      "Name: titre, dtype: object\n",
      "alcoholism\n",
      "15584    Shameless (British TV series)\n",
      "13729              I will moida da bum\n",
      "5827               Million Women Study\n",
      "8276          Violence and video games\n",
      "6156                      Chafing dish\n",
      "Name: titre, dtype: object\n",
      "amusement parks\n",
      "8584                            Hobby\n",
      "8758     Lists of tourist attractions\n",
      "14148                       Ice cream\n",
      "7748              Japanese war crimes\n",
      "10994                 Kenny McCormick\n",
      "Name: titre, dtype: object\n",
      "architecture\n",
      "750                          Art\n",
      "8750                      Museum\n",
      "12584    Heritage interpretation\n",
      "10584      Mapping controversies\n",
      "5584         Imaging informatics\n",
      "Name: titre, dtype: object\n",
      "bicycling\n",
      "6273                                        Pannier\n",
      "758                                  Conceptual art\n",
      "2827                                          Intel\n",
      "14841               Global Alliance for EcoMobility\n",
      "4843     List of Colorado geographic features lists\n",
      "Name: titre, dtype: object\n"
     ]
    }
   ],
   "execution_count": 42
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T23:09:08.733438Z",
     "start_time": "2024-05-21T23:08:55.918721Z"
    }
   },
   "source": [
    "for q in queries[:5]:\n",
    "    print(q)\n",
    "    print(score_normal_pagerank(q, topic_pagerank['business'])[:5])"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "affirmative action\n",
      "10992     List of Stargate Universe characters\n",
      "14017    Universal Declaration of Human Rights\n",
      "9991          Chronicle of the King D. Pedro I\n",
      "7101                               Jactitation\n",
      "748                              Theory of art\n",
      "Name: titre, dtype: object\n",
      "alcoholism\n",
      "6922                                   Skewer\n",
      "12922                            Public enemy\n",
      "11958                                  Sandoz\n",
      "8969                               Maxim Wien\n",
      "10992    List of Stargate Universe characters\n",
      "Name: titre, dtype: object\n",
      "amusement parks\n",
      "10958                          Tom Swift\n",
      "8136               Video games and Linux\n",
      "3996     Characters of the Tekken series\n",
      "8758        Lists of tourist attractions\n",
      "10994                    Kenny McCormick\n",
      "Name: titre, dtype: object\n",
      "architecture\n",
      "8962                                 Reserve design\n",
      "969                                          Design\n",
      "272                                       Patronage\n",
      "3272    List of Xbox games compatible with Xbox 360\n",
      "8750                                         Museum\n",
      "Name: titre, dtype: object\n",
      "bicycling\n",
      "2827                      Intel\n",
      "13955          Element (sports)\n",
      "8109         Humans vs. Zombies\n",
      "6273                    Pannier\n",
      "9302     Guinness World Records\n",
      "Name: titre, dtype: object\n"
     ]
    }
   ],
   "execution_count": 43
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T23:09:18.447306Z",
     "start_time": "2024-05-21T23:09:08.734693Z"
    }
   },
   "source": [
    "for q in queries[:5]:\n",
    "    print(q)\n",
    "    print(score_normal_pagerank(q, topic_pagerank['sports'])[:5])"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "affirmative action\n",
      "14017    Universal Declaration of Human Rights\n",
      "7810                           Murray Rothbard\n",
      "7101                               Jactitation\n",
      "10992     List of Stargate Universe characters\n",
      "14841          Global Alliance for EcoMobility\n",
      "Name: titre, dtype: object\n",
      "alcoholism\n",
      "5827              Million Women Study\n",
      "5934     Heart failure classification\n",
      "8969                       Maxim Wien\n",
      "11957                           Maggi\n",
      "5926                   Medical record\n",
      "Name: titre, dtype: object\n",
      "amusement parks\n",
      "8758     Lists of tourist attractions\n",
      "8584                            Hobby\n",
      "10958                       Tom Swift\n",
      "14148                       Ice cream\n",
      "10994                 Kenny McCormick\n",
      "Name: titre, dtype: object\n",
      "architecture\n",
      "750                                              Art\n",
      "8750                                          Museum\n",
      "2827                                           Intel\n",
      "8962                                  Reserve design\n",
      "10934    List of things named after John von Neumann\n",
      "Name: titre, dtype: object\n",
      "bicycling\n",
      "2827                                Intel\n",
      "1934                    Domar aggregation\n",
      "13955                    Element (sports)\n",
      "6273                              Pannier\n",
      "1700     American system of manufacturing\n",
      "Name: titre, dtype: object\n"
     ]
    }
   ],
   "execution_count": 44
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarité entre les ranking de pagerank normal et de pagerank avec topic"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T23:11:40.913742Z",
     "start_time": "2024-05-21T23:09:32.121662Z"
    }
   },
   "source": [
    "for q in queries:\n",
    "    normal = score_normal_pagerank(q, normal_pagerank)\n",
    "    topic = sqd(q, topic_pagerank, D, terms_indexes, index)\n",
    "    print(q,\"\\tOSim:\",OSim(normal, topic, n=20))\n",
    "    #print(\"KSim,\",KSim(normal[:20], topic[:20]))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "affirmative action \tOSim: 0.1\n",
      "alcoholism \tOSim: 0.9\n",
      "amusement parks \tOSim: 0.2\n",
      "architecture \tOSim: 1.0\n",
      "bicycling \tOSim: 0.9\n",
      "blues \tOSim: 0.85\n",
      "cheese \tOSim: 0.7\n",
      "citrus groves \tOSim: 0.15\n",
      "classical guitar \tOSim: 0.55\n",
      "computer vision \tOSim: 0.1\n",
      "cruises \tOSim: 0.75\n",
      "death valley \tOSim: 0.2\n",
      "field hockey \tOSim: 0.15\n",
      "gardening \tOSim: 0.6\n",
      "graphic design \tOSim: 0.0\n",
      "gulf war \tOSim: 0.35\n",
      "hiv \tOSim: 0.8\n",
      "java \tOSim: 0.85\n",
      "lipari \tOSim: 0.05\n",
      "lyme disease \tOSim: 0.2\n",
      "mutual funds \tOSim: 0.15\n",
      "national parks \tOSim: 0.0\n",
      "parallel architecture \tOSim: 0.3\n",
      "recycling cans \tOSim: 0.0\n",
      "rock climbing \tOSim: 0.55\n",
      "san francisco \tOSim: 0.0\n",
      "shakespeare \tOSim: 0.95\n",
      "stamp collecting \tOSim: 0.3\n",
      "sushi \tOSim: 0.85\n",
      "table tennis \tOSim: 0.35\n",
      "telecommuting \tOSim: 0.1\n",
      "vintage cars \tOSim: 0.9\n",
      "volcano \tOSim: 0.85\n",
      "zen buddhism \tOSim: 0.5\n",
      "zener \tOSim: 0.15\n"
     ]
    }
   ],
   "execution_count": 46
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T23:19:01.901221Z",
     "start_time": "2024-05-21T23:16:56.272273Z"
    }
   },
   "source": [
    "for q in queries:\n",
    "    normal = score_normal_pagerank(q, normal_pagerank)\n",
    "    topic = sqd(q, topic_pagerank, D, terms_indexes, index)\n",
    "    print(q,\"\\tKSim,\",KSim(normal[:20], topic[:20]))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "affirmative action \tKSim, 0.2927927927927928\n",
      "alcoholism \tKSim, 0.8484848484848485\n",
      "amusement parks \tKSim, 0.3548387096774194\n",
      "architecture \tKSim, 0.9526315789473684\n",
      "bicycling \tKSim, 0.8831168831168831\n",
      "blues \tKSim, 0.8023715415019763\n",
      "cheese \tKSim, 0.7476923076923077\n",
      "citrus groves \tKSim, 0.3333333333333333\n",
      "classical guitar \tKSim, 0.45454545454545453\n",
      "computer vision \tKSim, 0.24444444444444444\n",
      "cruises \tKSim, 0.6633333333333333\n",
      "death valley \tKSim, 0.2838709677419355\n",
      "field hockey \tKSim, 0.3092436974789916\n",
      "gardening \tKSim, 0.5079365079365079\n",
      "graphic design \tKSim, 0.21923076923076923\n",
      "gulf war \tKSim, 0.3314393939393939\n",
      "hiv \tKSim, 0.8268398268398268\n",
      "java \tKSim, 0.7707509881422925\n",
      "lipari \tKSim, 0\n",
      "lyme disease \tKSim, 0.6666666666666666\n",
      "mutual funds \tKSim, 0.2702702702702703\n",
      "national parks \tKSim, 0.23616734143049933\n",
      "parallel architecture \tKSim, 0.41935483870967744\n",
      "recycling cans \tKSim, 0.26282051282051283\n",
      "rock climbing \tKSim, 0.42610837438423643\n",
      "san francisco \tKSim, 0.23076923076923078\n",
      "shakespeare \tKSim, 0.8333333333333334\n",
      "stamp collecting \tKSim, 0.2838709677419355\n",
      "sushi \tKSim, 0.7575757575757576\n",
      "table tennis \tKSim, 0.2989247311827957\n",
      "telecommuting \tKSim, 1.0\n",
      "vintage cars \tKSim, 0.5584415584415584\n",
      "volcano \tKSim, 0.8771929824561403\n",
      "zen buddhism \tKSim, 0.7333333333333333\n",
      "zener \tKSim, 1.0\n"
     ]
    }
   ],
   "execution_count": 51
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparaison des vecteurs PageRank (table 4)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T23:53:55.092520Z",
     "start_time": "2024-05-21T23:53:55.090204Z"
    }
   },
   "source": [
    "pageranks = topic_pagerank.copy()\n",
    "pageranks[\"normal\"] = normal_pagerank"
   ],
   "outputs": [],
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-22T00:14:36.006194Z",
     "start_time": "2024-05-21T23:59:24.816664Z"
    }
   },
   "source": [
    "import json\n",
    "\n",
    "table2={}\n",
    "\n",
    "# for topic in Topic_names + [\"normal\"]:\n",
    "#     with open(f\"Rankings/{topic}.json\", 'w') as f:\n",
    "#         rankings = {}\n",
    "#         for q in queries:\n",
    "#             rankings[q] = score_normal_pagerank(q, pageranks[topic])[:20].tolist()\n",
    "#         json.dump(rankings, f, indent=4)\n",
    "\n",
    "for topic_1,topic_2 in list(itertools.permutations([\"normal\"]+Topic_names, 2)):\n",
    "    print(topic_1,topic_2)\n",
    "    sim = 0\n",
    "    \n",
    "    with open(f\"Rankings/{topic_1}.json\", 'r') as f:\n",
    "        rankings1 = json.load(f)\n",
    "    with open(f\"Rankings/{topic_2}.json\", 'r') as f:\n",
    "        rankings2 = json.load(f)\n",
    "    \n",
    "    for q in queries:\n",
    "        indexation1 = rankings1[q]\n",
    "        indexation2 = rankings2[q]\n",
    "        sim += KSim(indexation1, indexation2)\n",
    "    sim /= len(queries)\n",
    "    table2 = {f\"{topic_1}/{topic_2}\": sim}\n",
    "\n",
    "    "
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normal arts\n",
      "normal games\n",
      "normal youth\n",
      "normal reference\n",
      "normal shopping\n",
      "normal business\n",
      "normal health\n",
      "normal news\n",
      "normal geography\n",
      "normal society\n",
      "normal computers\n",
      "normal home\n",
      "normal recreation\n",
      "normal science\n",
      "normal sports\n",
      "normal world\n",
      "arts normal\n",
      "arts games\n",
      "arts youth\n",
      "arts reference\n",
      "arts shopping\n",
      "arts business\n",
      "arts health\n",
      "arts news\n",
      "arts geography\n",
      "arts society\n",
      "arts computers\n",
      "arts home\n",
      "arts recreation\n",
      "arts science\n",
      "arts sports\n",
      "arts world\n",
      "games normal\n",
      "games arts\n",
      "games youth\n",
      "games reference\n",
      "games shopping\n",
      "games business\n",
      "games health\n",
      "games news\n",
      "games geography\n",
      "games society\n",
      "games computers\n",
      "games home\n",
      "games recreation\n",
      "games science\n",
      "games sports\n",
      "games world\n",
      "youth normal\n",
      "youth arts\n",
      "youth games\n",
      "youth reference\n",
      "youth shopping\n",
      "youth business\n",
      "youth health\n",
      "youth news\n",
      "youth geography\n",
      "youth society\n",
      "youth computers\n",
      "youth home\n",
      "youth recreation\n",
      "youth science\n",
      "youth sports\n",
      "youth world\n",
      "reference normal\n",
      "reference arts\n",
      "reference games\n",
      "reference youth\n",
      "reference shopping\n",
      "reference business\n",
      "reference health\n",
      "reference news\n",
      "reference geography\n",
      "reference society\n",
      "reference computers\n",
      "reference home\n",
      "reference recreation\n",
      "reference science\n",
      "reference sports\n",
      "reference world\n",
      "shopping normal\n",
      "shopping arts\n",
      "shopping games\n",
      "shopping youth\n",
      "shopping reference\n",
      "shopping business\n",
      "shopping health\n",
      "shopping news\n",
      "shopping geography\n",
      "shopping society\n",
      "shopping computers\n",
      "shopping home\n",
      "shopping recreation\n",
      "shopping science\n",
      "shopping sports\n",
      "shopping world\n",
      "business normal\n",
      "business arts\n",
      "business games\n",
      "business youth\n",
      "business reference\n",
      "business shopping\n",
      "business health\n",
      "business news\n",
      "business geography\n",
      "business society\n",
      "business computers\n",
      "business home\n",
      "business recreation\n",
      "business science\n",
      "business sports\n",
      "business world\n",
      "health normal\n",
      "health arts\n",
      "health games\n",
      "health youth\n",
      "health reference\n",
      "health shopping\n",
      "health business\n",
      "health news\n",
      "health geography\n",
      "health society\n",
      "health computers\n",
      "health home\n",
      "health recreation\n",
      "health science\n",
      "health sports\n",
      "health world\n",
      "news normal\n",
      "news arts\n",
      "news games\n",
      "news youth\n",
      "news reference\n",
      "news shopping\n",
      "news business\n",
      "news health\n",
      "news geography\n",
      "news society\n",
      "news computers\n",
      "news home\n",
      "news recreation\n",
      "news science\n",
      "news sports\n",
      "news world\n",
      "geography normal\n",
      "geography arts\n",
      "geography games\n",
      "geography youth\n",
      "geography reference\n",
      "geography shopping\n",
      "geography business\n",
      "geography health\n",
      "geography news\n",
      "geography society\n",
      "geography computers\n",
      "geography home\n",
      "geography recreation\n",
      "geography science\n",
      "geography sports\n",
      "geography world\n",
      "society normal\n",
      "society arts\n",
      "society games\n",
      "society youth\n",
      "society reference\n",
      "society shopping\n",
      "society business\n",
      "society health\n",
      "society news\n",
      "society geography\n",
      "society computers\n",
      "society home\n",
      "society recreation\n",
      "society science\n",
      "society sports\n",
      "society world\n",
      "computers normal\n",
      "computers arts\n",
      "computers games\n",
      "computers youth\n",
      "computers reference\n",
      "computers shopping\n",
      "computers business\n",
      "computers health\n",
      "computers news\n",
      "computers geography\n",
      "computers society\n",
      "computers home\n",
      "computers recreation\n",
      "computers science\n",
      "computers sports\n",
      "computers world\n",
      "home normal\n",
      "home arts\n",
      "home games\n",
      "home youth\n",
      "home reference\n",
      "home shopping\n",
      "home business\n",
      "home health\n",
      "home news\n",
      "home geography\n",
      "home society\n",
      "home computers\n",
      "home recreation\n",
      "home science\n",
      "home sports\n",
      "home world\n",
      "recreation normal\n",
      "recreation arts\n",
      "recreation games\n",
      "recreation youth\n",
      "recreation reference\n",
      "recreation shopping\n",
      "recreation business\n",
      "recreation health\n",
      "recreation news\n",
      "recreation geography\n",
      "recreation society\n",
      "recreation computers\n",
      "recreation home\n",
      "recreation science\n",
      "recreation sports\n",
      "recreation world\n",
      "science normal\n",
      "science arts\n",
      "science games\n",
      "science youth\n",
      "science reference\n",
      "science shopping\n",
      "science business\n",
      "science health\n",
      "science news\n",
      "science geography\n",
      "science society\n",
      "science computers\n",
      "science home\n",
      "science recreation\n",
      "science sports\n",
      "science world\n",
      "sports normal\n",
      "sports arts\n",
      "sports games\n",
      "sports youth\n",
      "sports reference\n",
      "sports shopping\n",
      "sports business\n",
      "sports health\n",
      "sports news\n",
      "sports geography\n",
      "sports society\n",
      "sports computers\n",
      "sports home\n",
      "sports recreation\n",
      "sports science\n",
      "sports world\n",
      "world normal\n",
      "world arts\n",
      "world games\n",
      "world youth\n",
      "world reference\n",
      "world shopping\n",
      "world business\n",
      "world health\n",
      "world news\n",
      "world geography\n",
      "world society\n",
      "world computers\n",
      "world home\n",
      "world recreation\n",
      "world science\n",
      "world sports\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
