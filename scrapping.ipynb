{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install wikipedia-api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wikipediaapi\n",
    "\n",
    "\n",
    "wiki_wiki = wikipediaapi.Wikipedia(\n",
    "    user_agent='MyProjectName',\n",
    "        language='en',\n",
    "        extract_format=wikipediaapi.ExtractFormat.WIKI\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Get data for each category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import shuffle\n",
    "\n",
    "\n",
    "def url_categorymembers(categorymembers,list_pages=[], level=0, max_level=3, i=0):\n",
    "    pages_cat = list(categorymembers.values()) #toutes les pages de la catégorie\n",
    "    shuffle(pages_cat)\n",
    "\n",
    "    for c in pages_cat:\n",
    "        if i==1000: #1000 pages par catégorie\n",
    "            break\n",
    "\n",
    "        if c.title[:6]!=\"Portal\" and c.title[:5]!=\"File:\" and c.title[:8]!=\"Category\": # vérifie que c'est bien un article\n",
    "            i+=1\n",
    "            d = {}\n",
    "            d[\"titre\"] = c.title\n",
    "            d[\"contenu\"] = c.text\n",
    "            d[\"liens\"] = list(c.links.keys())\n",
    "            list_pages.append(d)\n",
    "\n",
    "        if c.ns == wikipediaapi.Namespace.CATEGORY and level < max_level: # descend d'un niveau\n",
    "            i = url_categorymembers(c.categorymembers, list_pages, level=level + 1, max_level=max_level, i=i)[1]\n",
    "\n",
    "    return list_pages, i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_arts = wiki_wiki.page(\"Category:The arts\") # Arts\n",
    "arts, _ = url_categorymembers(cat_arts.categorymembers)\n",
    "pd.DataFrame(arts).to_csv('arts.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_games = wiki_wiki.page(\"Category:Games\") # Games\n",
    "games, _= url_categorymembers(cat_games.categorymembers)\n",
    "\n",
    "pd.DataFrame(games).to_csv('games.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_youth = wiki_wiki.page(\"Category:Youth\") # Kids and Teens (not exact)\n",
    "youth, _ = url_categorymembers(cat_youth.categorymembers)\n",
    "\n",
    "pd.DataFrame(youth).to_csv('youth.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_reference = wiki_wiki.page(\"Category:Reference\") # Reference\n",
    "reference, _ = url_categorymembers(cat_reference.categorymembers)\n",
    "\n",
    "pd.DataFrame(reference).to_csv('reference.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_shopping = wiki_wiki.page(\"Category:Retailing\") # Shopping\n",
    "shopping, _ = url_categorymembers(cat_shopping.categorymembers)\n",
    "\n",
    "pd.DataFrame(shopping).to_csv('shopping.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_business = wiki_wiki.page(\"Category:Business\") # Business\n",
    "business, _ = url_categorymembers(cat_business.categorymembers)\n",
    "\n",
    "pd.DataFrame(business).to_csv('business.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_health = wiki_wiki.page(\"Category:Health\") # Health\n",
    "health, _ = url_categorymembers(cat_health.categorymembers)\n",
    "\n",
    "pd.DataFrame(health).to_csv('health.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_news = wiki_wiki.page(\"Category:News\") # News\n",
    "news, _ = url_categorymembers(cat_news.categorymembers)\n",
    "\n",
    "pd.DataFrame(news).to_csv('news.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_geography = wiki_wiki.page(\"Category:Geography\") # Regional (not exact)\n",
    "geography, _ = url_categorymembers(cat_geography.categorymembers)\n",
    "\n",
    "pd.DataFrame(geography).to_csv('geography.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_society = wiki_wiki.page(\"Category:Society\") # Society\n",
    "society, _ = url_categorymembers(cat_society.categorymembers)\n",
    "\n",
    "pd.DataFrame(society).to_csv('society.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_computers = wiki_wiki.page(\"Category:Computers\") # Computers\n",
    "computers, _ = url_categorymembers(cat_computers.categorymembers)\n",
    "\n",
    "pd.DataFrame(computers).to_csv('computers.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_home = wiki_wiki.page(\"Category:Home\") # Home\n",
    "home, _ = url_categorymembers(cat_home.categorymembers)\n",
    "\n",
    "pd.DataFrame(home).to_csv('home.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_recreation = wiki_wiki.page(\"Category:Recreation\") # Recreation\n",
    "recreation, _ = url_categorymembers(cat_recreation.categorymembers)\n",
    "\n",
    "pd.DataFrame(recreation).to_csv('recreation.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_science = wiki_wiki.page(\"Category:Science\") # Science\n",
    "science, _ = url_categorymembers(cat_science.categorymembers)\n",
    "\n",
    "pd.DataFrame(science).to_csv('science.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_sports = wiki_wiki.page(\"Category:Sports\") # Sports\n",
    "sports, _ = url_categorymembers(cat_sports.categorymembers)\n",
    "\n",
    "pd.DataFrame(sports).to_csv('sports.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_world = wiki_wiki.page(\"Category:World\") # World\n",
    "world, _ = url_categorymembers(cat_world.categorymembers)\n",
    "\n",
    "pd.DataFrame(world).to_csv('world.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_files = [\n",
    "    'arts.csv', 'business.csv', 'computers.csv', 'games.csv',\n",
    "    'geography.csv', 'health.csv', 'home.csv', 'news.csv',\n",
    "    'recreation.csv', 'reference.csv', 'science.csv', 'shopping.csv',\n",
    "    'society.csv', 'sports.csv', 'world.csv', 'youth.csv'\n",
    "]\n",
    "\n",
    "# Liste pour stocker les DataFrames\n",
    "dataframes = []\n",
    "\n",
    "# Lire chaque fichier CSV et l'ajouter à la liste des DataFrames\n",
    "for file in csv_files:\n",
    "    df = pd.read_csv(f'./Data/{file}')\n",
    "    dataframes.append(df)\n",
    "\n",
    "# Combiner tous les DataFrames en un seul\n",
    "combined_df = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "# Écrire le DataFrame combiné dans un nouveau fichier CSV\n",
    "combined_df.to_csv('./Data/all.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute Page Rank vectors (3.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrices d'adjacence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construction de la matrice sans utiliser les fichiers :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the adjacency matrix\n",
    "def build_adjacency_matrix_2(df): #pas optimisé\n",
    "    num_pages = df.shape[0]\n",
    "    adjacency_matrix = np.zeros((num_pages, num_pages))\n",
    "    df[\"liens\"] = df[\"liens\"].apply(ast.literal_eval)\n",
    "    \n",
    "    for index, row in df.iterrows():\n",
    "        N = len(np.where(np.isin(row['liens'],df['titre'].values))[0]) # nombres de liens que fait la page\n",
    "        for link in row['liens']:\n",
    "            if link in df['titre'].values:\n",
    "                j = np.where(df['titre'] == link )[0]\n",
    "                adjacency_matrix[j,index] += 1/N\n",
    "\n",
    "    return adjacency_matrix\n",
    "\n",
    "\n",
    "\n",
    "def build_adjacency_matrix(df):\n",
    "    df[\"liens\"] = df[\"liens\"].apply(ast.literal_eval)  # Convertir les liens de type string en listes\n",
    "\n",
    "    # Créer un dictionnaire pour mapper les titres aux indices\n",
    "    title_to_index = {title: i for i, title in enumerate(df['titre'])}\n",
    "    \n",
    "    # Initialiser la matrice d'adjacence avec des zéros\n",
    "    num_pages = len(df)\n",
    "    adjacency_matrix = np.zeros((num_pages, num_pages))\n",
    "    \n",
    "    # Construire une série pour mapper les liens aux indices\n",
    "    link_series = pd.Series(df['liens'].values.tolist())\n",
    "    link_indices = link_series.map(lambda x: [title_to_index[link] for link in x if link in title_to_index])\n",
    "    \n",
    "    # Compter le nombre de liens sortants pour chaque article\n",
    "    df['num_liens'] = link_indices.apply(len)\n",
    "    \n",
    "    for i, indices in enumerate(link_indices):\n",
    "        if len(indices)>0:\n",
    "            adjacency_matrix[indices, i] = 1 / df.at[i, 'num_liens']\n",
    "    \n",
    "    return adjacency_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrice_all= build_adjacency_matrix(pd.read_csv('./Data/all.csv'))\n",
    "np.save('./Matrices/matrice_all.npy', matrice_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construction de la matrice en utilisant les fichiers :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fichier_graphe = pd.read_csv('enwiki.wikilink_graph.2018-03-01.csv')\n",
    "\n",
    "# build the adjacency matrix\n",
    "def build_adjacency_matrix(topic_articles, inversed_index_articles):\n",
    "    num_pages = len(topic_articles)\n",
    "    adjacency_matrix = np.zeros((num_pages, num_pages))\n",
    "\n",
    "    for i,titre in enumerate(topic_articles):\n",
    "        #links = page_links(pages[i])\n",
    "        links = fichier_graphe[fichier_graphe['page_title_from']==titre]\n",
    "        for link in links:\n",
    "            if link in topic_articles:\n",
    "                j = topic_articles.index(link)\n",
    "                adjacency_matrix[j][i] = 1\n",
    "\n",
    "    return adjacency_matrix/sum(adjacency_matrix,axis=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calcul des vecteurs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute PageRank scores\n",
    "def compute_pagerank(M, alpha=0.25, bias = None): # bias = None pour page_rank normal, bias = topic sinon\n",
    "    # M = adjacency_matrix\n",
    "    # damping_factor = 1-alpha\n",
    "\n",
    "    N = M.shape[0]\n",
    "\n",
    "    if bias is None : # page_rank normal, pas topic_sensitive\n",
    "        p = np.ones(N) / N \n",
    "\n",
    "    else: # ODP-biasing \n",
    "        articles = pd.read_csv(f'./Data/all.csv')[\"titre\"].values\n",
    "        articles_cat = pd.read_csv(f'./Data/{bias}.csv')[\"titre\"].values\n",
    "        p = np.where(np.isin(articles,articles_cat), 1/len(articles_cat), 0)\n",
    "\n",
    "    #M_prime = (1-alpha) * M + alpha * teleportation_matrix\n",
    "\n",
    "    rank = np.ones(N) / N\n",
    "    old_rank = np.zeros(N)\n",
    "\n",
    "    epsilon = 1.0e-3\n",
    "    max_iterations = 10\n",
    "    iterations = 0\n",
    "\n",
    "    while np.sum(np.abs(rank - old_rank)) > epsilon and iterations < max_iterations:\n",
    "        old_rank = rank.copy()\n",
    "        rank = (1-alpha)*M @ rank + p # equation 5\n",
    "        #normaliser ?\n",
    "        iterations += 1\n",
    "\n",
    "    return rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: arts\n",
      "[0.001      0.00244676 0.00642531 0.00290021 0.00235192]\n",
      "Topic: games\n",
      "[0.00000000e+00 2.35503033e-06 1.60950517e-05 2.47080165e-06\n",
      " 2.07752981e-05]\n",
      "Topic: youth\n",
      "[0.00000000e+00 5.27069792e-06 3.56002031e-05 5.47862884e-06\n",
      " 1.82830858e-04]\n",
      "Topic: reference\n",
      "[0.00000000e+00 1.58029940e-05 1.10839427e-04 1.64222844e-05\n",
      " 4.09560939e-04]\n",
      "Topic: shopping\n",
      "[0.00000000e+00 8.11350040e-06 5.25511529e-05 8.44755001e-06\n",
      " 2.59608471e-05]\n",
      "Topic: business\n",
      "[0.00000000e+00 7.12745567e-06 2.65537639e-05 7.39839860e-06\n",
      " 3.32621253e-05]\n",
      "Topic: health\n",
      "[0.00000000e+00 5.26679217e-06 1.16601897e-05 5.49715984e-06\n",
      " 1.62895229e-05]\n",
      "Topic: news\n",
      "[0.00000000e+00 6.51077029e-06 3.63164055e-05 6.76543462e-06\n",
      " 4.22531048e-04]\n",
      "Topic: geography\n",
      "[0.00000000e+00 8.34079457e-06 5.01055461e-05 8.66262866e-06\n",
      " 2.41785777e-04]\n",
      "Topic: society\n",
      "[0.00000000e+00 2.29043309e-05 7.75766209e-05 2.37939236e-05\n",
      " 2.48250996e-04]\n",
      "Topic: computers\n",
      "[0.00000000e+00 2.28773524e-06 1.26314431e-05 2.39396011e-06\n",
      " 1.49644850e-05]\n",
      "Topic: home\n",
      "[0.00000000e+00 1.00987472e-05 7.97914850e-05 1.04980206e-05\n",
      " 2.90179168e-05]\n",
      "Topic: recreation\n",
      "[0.00000000e+00 8.00333412e-06 5.89331913e-05 8.32091978e-06\n",
      " 9.25937248e-05]\n",
      "Topic: science\n",
      "[0.00000000e+00 9.05813817e-06 4.53954414e-05 9.41049148e-06\n",
      " 1.55695334e-04]\n",
      "Topic: sports\n",
      "[0.00000000e+00 5.23818191e-05 4.06730657e-04 5.46668548e-05\n",
      " 5.17132600e-05]\n",
      "Topic: world\n",
      "[0.00000000e+00 3.88539053e-06 1.77705041e-05 4.04065748e-06\n",
      " 1.92726381e-05]\n"
     ]
    }
   ],
   "source": [
    "# Compute PageRank scores for each topic\n",
    "import numpy as np\n",
    "\n",
    "Topic_names = [\"arts\",\"games\",\"youth\",\"reference\", \"shopping\", \"business\", \"health\", \"news\",\"geography\",\"society\",\"computers\",\"home\",\"recreation\",\"science\",\"sports\",\"world\"]\n",
    "\n",
    "adjacency_matrix = np.load(f'./Matrices/matrice_all.npy')\n",
    "\n",
    "topic_pagerank = {}\n",
    "for topic in Topic_names:\n",
    "    #adjacency_matrix = np.load(f'./Matrices/matrice_{topic}.npy')\n",
    "    \n",
    "    pagerank_scores = compute_pagerank(adjacency_matrix,bias=topic)\n",
    "\n",
    "    print(f\"Topic: {topic}\")\n",
    "    print(pagerank_scores[:5])\n",
    "\n",
    "    topic_pagerank[topic] = pagerank_scores\n",
    "\n",
    "    "
   ],
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[4], line 5\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# Compute PageRank scores for each topic\u001B[39;00m\n\u001B[1;32m      3\u001B[0m Topic_names \u001B[38;5;241m=\u001B[39m [\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124marts\u001B[39m\u001B[38;5;124m\"\u001B[39m,\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mgames\u001B[39m\u001B[38;5;124m\"\u001B[39m,\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124myouth\u001B[39m\u001B[38;5;124m\"\u001B[39m,\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mreference\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mshopping\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbusiness\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhealth\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnews\u001B[39m\u001B[38;5;124m\"\u001B[39m,\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mgeography\u001B[39m\u001B[38;5;124m\"\u001B[39m,\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msociety\u001B[39m\u001B[38;5;124m\"\u001B[39m,\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcomputers\u001B[39m\u001B[38;5;124m\"\u001B[39m,\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhome\u001B[39m\u001B[38;5;124m\"\u001B[39m,\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrecreation\u001B[39m\u001B[38;5;124m\"\u001B[39m,\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mscience\u001B[39m\u001B[38;5;124m\"\u001B[39m,\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msports\u001B[39m\u001B[38;5;124m\"\u001B[39m,\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mworld\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[0;32m----> 5\u001B[0m adjacency_matrix \u001B[38;5;241m=\u001B[39m \u001B[43mnp\u001B[49m\u001B[38;5;241m.\u001B[39mload(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m./Matrices/matrice_all.npy\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m      7\u001B[0m topic_pagerank \u001B[38;5;241m=\u001B[39m {}\n\u001B[1;32m      8\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m topic \u001B[38;5;129;01min\u001B[39;00m Topic_names:\n\u001B[1;32m      9\u001B[0m     \u001B[38;5;66;03m#adjacency_matrix = np.load(f'./Matrices/matrice_{topic}.npy')\u001B[39;00m\n",
      "\u001B[0;31mNameError\u001B[0m: name 'np' is not defined"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('topic_pagerank.pkl', 'wb') as fichier:\n",
    "    pickle.dump(topic_pagerank, fichier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adjacency_matrix = np.load(f'./Matrices/matrice_all.npy')\n",
    "pagerank_normal = compute_pagerank(adjacency_matrix,bias=None)\n",
    "with open('normal_pagerank.pkl', 'wb') as fichier:\n",
    "    pickle.dump(pagerank_normal, fichier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query-time importance score (3.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calcul de D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "\n",
    "categories_text = {}\n",
    "for c in Topic_names:\n",
    "    liste_textes = pd.read_csv(f'./Data/{c}.csv')[\"contenu\"].values.astype(str)\n",
    "\n",
    "    texte = ' '.join(liste_textes)\n",
    "    categories_text[c] = texte\n",
    "\n",
    "D_categories = {}\n",
    "terms_indexes_categories = {}\n",
    "\n",
    "for c, text in categories_text.items():\n",
    "    vectorizer = CountVectorizer(stop_words='english')\n",
    "    D_categories[c] = vectorizer.fit_transform([text])\n",
    "    terms_indexes_categories[c] = vectorizer.get_feature_names_out()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('D.pkl', 'wb') as fichier:\n",
    "    pickle.dump(D_categories, fichier)\n",
    "\n",
    "with open('terms_indexes.pkl', 'wb') as fichier:\n",
    "    pickle.dump(terms_indexes_categories, fichier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install python-terrier"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T21:11:23.056539Z",
     "start_time": "2024-05-21T21:11:01.587187Z"
    }
   },
   "source": [
    "# Indexation des articles \n",
    "import pandas as pd\n",
    "import pyterrier as pt\n",
    "\n",
    "if not pt.started():\n",
    "  pt.init(boot_packages=[\"com.github.terrierteam:terrier-prf:-SNAPSHOT\"])\n",
    "\n",
    "df = pd.read_csv(f'./Data/all.csv')[[\"titre\",\"contenu\"]]\n",
    "df = df.dropna()\n",
    "df.columns = [\"docno\", \"text\"]\n",
    "\n",
    "pd_indexer = pt.DFIndexer(\"./pd_index\")\n",
    "indexref = pd_indexer.index(df[\"text\"], df[\"docno\"])\n",
    "index = pt.IndexFactory.of(indexref)\n",
    "\n",
    "Topic_names = [\"arts\",\"games\",\"youth\",\"reference\", \"shopping\", \"business\", \"health\", \"news\",\"geography\",\"society\",\"computers\",\"home\",\"recreation\",\"science\",\"sports\",\"world\"]"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23:11:06.130 [main] WARN org.terrier.structures.indexing.Indexer - skipping null document\n",
      "23:11:06.258 [main] WARN org.terrier.structures.indexing.Indexer - skipping null document\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"jnius/jnius_proxy.pxi\", line 50, in jnius.PythonJavaClass.invoke\n",
      "  File \"jnius/jnius_proxy.pxi\", line 76, in jnius.PythonJavaClass._invoke\n",
      "  File \"/opt/homebrew/lib/python3.11/site-packages/pyterrier/index.py\", line 778, in next\n",
      "    lastdoc = self.convertFn(text, meta)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/lib/python3.11/site-packages/pyterrier/index.py\", line 687, in convertDoc\n",
      "    return TaggedDocument(StringReader(text_row), hashmap, Tokeniser.getTokeniser())\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"jnius/jnius_export_class.pxi\", line 269, in jnius.JavaClass.__init__\n",
      "  File \"jnius/jnius_export_class.pxi\", line 362, in jnius.JavaClass.call_constructor\n",
      "  File \"jnius/jnius_conversion.pxi\", line 74, in jnius.populate_args\n",
      "  File \"jnius/jnius_utils.pxi\", line 193, in jnius.check_assignable_from_str\n",
      "TypeError: Invalid instance of 'java/lang/Float' passed for a 'java/lang/String'\n",
      "Traceback (most recent call last):\n",
      "  File \"jnius/jnius_proxy.pxi\", line 50, in jnius.PythonJavaClass.invoke\n",
      "  File \"jnius/jnius_proxy.pxi\", line 76, in jnius.PythonJavaClass._invoke\n",
      "  File \"/opt/homebrew/lib/python3.11/site-packages/pyterrier/index.py\", line 778, in next\n",
      "    lastdoc = self.convertFn(text, meta)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/lib/python3.11/site-packages/pyterrier/index.py\", line 687, in convertDoc\n",
      "    return TaggedDocument(StringReader(text_row), hashmap, Tokeniser.getTokeniser())\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"jnius/jnius_export_class.pxi\", line 269, in jnius.JavaClass.__init__\n",
      "  File \"jnius/jnius_export_class.pxi\", line 362, in jnius.JavaClass.call_constructor\n",
      "  File \"jnius/jnius_conversion.pxi\", line 74, in jnius.populate_args\n",
      "  File \"jnius/jnius_utils.pxi\", line 193, in jnius.check_assignable_from_str\n",
      "TypeError: Invalid instance of 'java/lang/Float' passed for a 'java/lang/String'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23:11:06.335 [main] WARN org.terrier.structures.indexing.Indexer - skipping null document\n",
      "23:11:06.348 [main] WARN org.terrier.structures.indexing.Indexer - skipping null document\n",
      "23:11:06.356 [main] WARN org.terrier.structures.indexing.Indexer - skipping null document\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"jnius/jnius_proxy.pxi\", line 50, in jnius.PythonJavaClass.invoke\n",
      "  File \"jnius/jnius_proxy.pxi\", line 76, in jnius.PythonJavaClass._invoke\n",
      "  File \"/opt/homebrew/lib/python3.11/site-packages/pyterrier/index.py\", line 778, in next\n",
      "    lastdoc = self.convertFn(text, meta)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/lib/python3.11/site-packages/pyterrier/index.py\", line 687, in convertDoc\n",
      "    return TaggedDocument(StringReader(text_row), hashmap, Tokeniser.getTokeniser())\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"jnius/jnius_export_class.pxi\", line 269, in jnius.JavaClass.__init__\n",
      "  File \"jnius/jnius_export_class.pxi\", line 362, in jnius.JavaClass.call_constructor\n",
      "  File \"jnius/jnius_conversion.pxi\", line 74, in jnius.populate_args\n",
      "  File \"jnius/jnius_utils.pxi\", line 193, in jnius.check_assignable_from_str\n",
      "TypeError: Invalid instance of 'java/lang/Float' passed for a 'java/lang/String'\n",
      "Traceback (most recent call last):\n",
      "  File \"jnius/jnius_proxy.pxi\", line 50, in jnius.PythonJavaClass.invoke\n",
      "  File \"jnius/jnius_proxy.pxi\", line 76, in jnius.PythonJavaClass._invoke\n",
      "  File \"/opt/homebrew/lib/python3.11/site-packages/pyterrier/index.py\", line 778, in next\n",
      "    lastdoc = self.convertFn(text, meta)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/lib/python3.11/site-packages/pyterrier/index.py\", line 687, in convertDoc\n",
      "    return TaggedDocument(StringReader(text_row), hashmap, Tokeniser.getTokeniser())\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"jnius/jnius_export_class.pxi\", line 269, in jnius.JavaClass.__init__\n",
      "  File \"jnius/jnius_export_class.pxi\", line 362, in jnius.JavaClass.call_constructor\n",
      "  File \"jnius/jnius_conversion.pxi\", line 74, in jnius.populate_args\n",
      "  File \"jnius/jnius_utils.pxi\", line 193, in jnius.check_assignable_from_str\n",
      "TypeError: Invalid instance of 'java/lang/Float' passed for a 'java/lang/String'\n",
      "Traceback (most recent call last):\n",
      "  File \"jnius/jnius_proxy.pxi\", line 50, in jnius.PythonJavaClass.invoke\n",
      "  File \"jnius/jnius_proxy.pxi\", line 76, in jnius.PythonJavaClass._invoke\n",
      "  File \"/opt/homebrew/lib/python3.11/site-packages/pyterrier/index.py\", line 778, in next\n",
      "    lastdoc = self.convertFn(text, meta)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/lib/python3.11/site-packages/pyterrier/index.py\", line 687, in convertDoc\n",
      "    return TaggedDocument(StringReader(text_row), hashmap, Tokeniser.getTokeniser())\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"jnius/jnius_export_class.pxi\", line 269, in jnius.JavaClass.__init__\n",
      "  File \"jnius/jnius_export_class.pxi\", line 362, in jnius.JavaClass.call_constructor\n",
      "  File \"jnius/jnius_conversion.pxi\", line 74, in jnius.populate_args\n",
      "  File \"jnius/jnius_utils.pxi\", line 193, in jnius.check_assignable_from_str\n",
      "TypeError: Invalid instance of 'java/lang/Float' passed for a 'java/lang/String'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23:11:08.741 [main] WARN org.terrier.structures.indexing.Indexer - skipping null document\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"jnius/jnius_proxy.pxi\", line 50, in jnius.PythonJavaClass.invoke\n",
      "  File \"jnius/jnius_proxy.pxi\", line 76, in jnius.PythonJavaClass._invoke\n",
      "  File \"/opt/homebrew/lib/python3.11/site-packages/pyterrier/index.py\", line 778, in next\n",
      "    lastdoc = self.convertFn(text, meta)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/lib/python3.11/site-packages/pyterrier/index.py\", line 687, in convertDoc\n",
      "    return TaggedDocument(StringReader(text_row), hashmap, Tokeniser.getTokeniser())\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"jnius/jnius_export_class.pxi\", line 269, in jnius.JavaClass.__init__\n",
      "  File \"jnius/jnius_export_class.pxi\", line 362, in jnius.JavaClass.call_constructor\n",
      "  File \"jnius/jnius_conversion.pxi\", line 74, in jnius.populate_args\n",
      "  File \"jnius/jnius_utils.pxi\", line 193, in jnius.check_assignable_from_str\n",
      "TypeError: Invalid instance of 'java/lang/Float' passed for a 'java/lang/String'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23:11:09.447 [main] WARN org.terrier.structures.indexing.Indexer - skipping null document\n",
      "23:11:09.543 [main] WARN org.terrier.structures.indexing.Indexer - skipping null document\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"jnius/jnius_proxy.pxi\", line 50, in jnius.PythonJavaClass.invoke\n",
      "  File \"jnius/jnius_proxy.pxi\", line 76, in jnius.PythonJavaClass._invoke\n",
      "  File \"/opt/homebrew/lib/python3.11/site-packages/pyterrier/index.py\", line 778, in next\n",
      "    lastdoc = self.convertFn(text, meta)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/lib/python3.11/site-packages/pyterrier/index.py\", line 687, in convertDoc\n",
      "    return TaggedDocument(StringReader(text_row), hashmap, Tokeniser.getTokeniser())\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"jnius/jnius_export_class.pxi\", line 269, in jnius.JavaClass.__init__\n",
      "  File \"jnius/jnius_export_class.pxi\", line 362, in jnius.JavaClass.call_constructor\n",
      "  File \"jnius/jnius_conversion.pxi\", line 74, in jnius.populate_args\n",
      "  File \"jnius/jnius_utils.pxi\", line 193, in jnius.check_assignable_from_str\n",
      "TypeError: Invalid instance of 'java/lang/Float' passed for a 'java/lang/String'\n",
      "Traceback (most recent call last):\n",
      "  File \"jnius/jnius_proxy.pxi\", line 50, in jnius.PythonJavaClass.invoke\n",
      "  File \"jnius/jnius_proxy.pxi\", line 76, in jnius.PythonJavaClass._invoke\n",
      "  File \"/opt/homebrew/lib/python3.11/site-packages/pyterrier/index.py\", line 778, in next\n",
      "    lastdoc = self.convertFn(text, meta)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/lib/python3.11/site-packages/pyterrier/index.py\", line 687, in convertDoc\n",
      "    return TaggedDocument(StringReader(text_row), hashmap, Tokeniser.getTokeniser())\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"jnius/jnius_export_class.pxi\", line 269, in jnius.JavaClass.__init__\n",
      "  File \"jnius/jnius_export_class.pxi\", line 362, in jnius.JavaClass.call_constructor\n",
      "  File \"jnius/jnius_conversion.pxi\", line 74, in jnius.populate_args\n",
      "  File \"jnius/jnius_utils.pxi\", line 193, in jnius.check_assignable_from_str\n",
      "TypeError: Invalid instance of 'java/lang/Float' passed for a 'java/lang/String'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23:11:10.272 [main] WARN org.terrier.structures.indexing.Indexer - skipping null document\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"jnius/jnius_proxy.pxi\", line 50, in jnius.PythonJavaClass.invoke\n",
      "  File \"jnius/jnius_proxy.pxi\", line 76, in jnius.PythonJavaClass._invoke\n",
      "  File \"/opt/homebrew/lib/python3.11/site-packages/pyterrier/index.py\", line 778, in next\n",
      "    lastdoc = self.convertFn(text, meta)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/lib/python3.11/site-packages/pyterrier/index.py\", line 687, in convertDoc\n",
      "    return TaggedDocument(StringReader(text_row), hashmap, Tokeniser.getTokeniser())\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"jnius/jnius_export_class.pxi\", line 269, in jnius.JavaClass.__init__\n",
      "  File \"jnius/jnius_export_class.pxi\", line 362, in jnius.JavaClass.call_constructor\n",
      "  File \"jnius/jnius_conversion.pxi\", line 74, in jnius.populate_args\n",
      "  File \"jnius/jnius_utils.pxi\", line 193, in jnius.check_assignable_from_str\n",
      "TypeError: Invalid instance of 'java/lang/Float' passed for a 'java/lang/String'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23:11:10.716 [main] WARN org.terrier.structures.indexing.Indexer - skipping null document\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"jnius/jnius_proxy.pxi\", line 50, in jnius.PythonJavaClass.invoke\n",
      "  File \"jnius/jnius_proxy.pxi\", line 76, in jnius.PythonJavaClass._invoke\n",
      "  File \"/opt/homebrew/lib/python3.11/site-packages/pyterrier/index.py\", line 778, in next\n",
      "    lastdoc = self.convertFn(text, meta)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/lib/python3.11/site-packages/pyterrier/index.py\", line 687, in convertDoc\n",
      "    return TaggedDocument(StringReader(text_row), hashmap, Tokeniser.getTokeniser())\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"jnius/jnius_export_class.pxi\", line 269, in jnius.JavaClass.__init__\n",
      "  File \"jnius/jnius_export_class.pxi\", line 362, in jnius.JavaClass.call_constructor\n",
      "  File \"jnius/jnius_conversion.pxi\", line 74, in jnius.populate_args\n",
      "  File \"jnius/jnius_utils.pxi\", line 193, in jnius.check_assignable_from_str\n",
      "TypeError: Invalid instance of 'java/lang/Float' passed for a 'java/lang/String'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23:11:11.010 [main] WARN org.terrier.structures.indexing.Indexer - skipping null document\n",
      "23:11:11.029 [main] WARN org.terrier.structures.indexing.Indexer - skipping null document\n",
      "23:11:11.182 [main] WARN org.terrier.structures.indexing.Indexer - skipping null document\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"jnius/jnius_proxy.pxi\", line 50, in jnius.PythonJavaClass.invoke\n",
      "  File \"jnius/jnius_proxy.pxi\", line 76, in jnius.PythonJavaClass._invoke\n",
      "  File \"/opt/homebrew/lib/python3.11/site-packages/pyterrier/index.py\", line 778, in next\n",
      "    lastdoc = self.convertFn(text, meta)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/lib/python3.11/site-packages/pyterrier/index.py\", line 687, in convertDoc\n",
      "    return TaggedDocument(StringReader(text_row), hashmap, Tokeniser.getTokeniser())\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"jnius/jnius_export_class.pxi\", line 269, in jnius.JavaClass.__init__\n",
      "  File \"jnius/jnius_export_class.pxi\", line 362, in jnius.JavaClass.call_constructor\n",
      "  File \"jnius/jnius_conversion.pxi\", line 74, in jnius.populate_args\n",
      "  File \"jnius/jnius_utils.pxi\", line 193, in jnius.check_assignable_from_str\n",
      "TypeError: Invalid instance of 'java/lang/Float' passed for a 'java/lang/String'\n",
      "Traceback (most recent call last):\n",
      "  File \"jnius/jnius_proxy.pxi\", line 50, in jnius.PythonJavaClass.invoke\n",
      "  File \"jnius/jnius_proxy.pxi\", line 76, in jnius.PythonJavaClass._invoke\n",
      "  File \"/opt/homebrew/lib/python3.11/site-packages/pyterrier/index.py\", line 778, in next\n",
      "    lastdoc = self.convertFn(text, meta)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/lib/python3.11/site-packages/pyterrier/index.py\", line 687, in convertDoc\n",
      "    return TaggedDocument(StringReader(text_row), hashmap, Tokeniser.getTokeniser())\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"jnius/jnius_export_class.pxi\", line 269, in jnius.JavaClass.__init__\n",
      "  File \"jnius/jnius_export_class.pxi\", line 362, in jnius.JavaClass.call_constructor\n",
      "  File \"jnius/jnius_conversion.pxi\", line 74, in jnius.populate_args\n",
      "  File \"jnius/jnius_utils.pxi\", line 193, in jnius.check_assignable_from_str\n",
      "TypeError: Invalid instance of 'java/lang/Float' passed for a 'java/lang/String'\n",
      "Traceback (most recent call last):\n",
      "  File \"jnius/jnius_proxy.pxi\", line 50, in jnius.PythonJavaClass.invoke\n",
      "  File \"jnius/jnius_proxy.pxi\", line 76, in jnius.PythonJavaClass._invoke\n",
      "  File \"/opt/homebrew/lib/python3.11/site-packages/pyterrier/index.py\", line 778, in next\n",
      "    lastdoc = self.convertFn(text, meta)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/lib/python3.11/site-packages/pyterrier/index.py\", line 687, in convertDoc\n",
      "    return TaggedDocument(StringReader(text_row), hashmap, Tokeniser.getTokeniser())\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"jnius/jnius_export_class.pxi\", line 269, in jnius.JavaClass.__init__\n",
      "  File \"jnius/jnius_export_class.pxi\", line 362, in jnius.JavaClass.call_constructor\n",
      "  File \"jnius/jnius_conversion.pxi\", line 74, in jnius.populate_args\n",
      "  File \"jnius/jnius_utils.pxi\", line 193, in jnius.check_assignable_from_str\n",
      "TypeError: Invalid instance of 'java/lang/Float' passed for a 'java/lang/String'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23:11:12.433 [main] WARN org.terrier.structures.indexing.Indexer - skipping null document\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"jnius/jnius_proxy.pxi\", line 50, in jnius.PythonJavaClass.invoke\n",
      "  File \"jnius/jnius_proxy.pxi\", line 76, in jnius.PythonJavaClass._invoke\n",
      "  File \"/opt/homebrew/lib/python3.11/site-packages/pyterrier/index.py\", line 778, in next\n",
      "    lastdoc = self.convertFn(text, meta)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/lib/python3.11/site-packages/pyterrier/index.py\", line 687, in convertDoc\n",
      "    return TaggedDocument(StringReader(text_row), hashmap, Tokeniser.getTokeniser())\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"jnius/jnius_export_class.pxi\", line 269, in jnius.JavaClass.__init__\n",
      "  File \"jnius/jnius_export_class.pxi\", line 362, in jnius.JavaClass.call_constructor\n",
      "  File \"jnius/jnius_conversion.pxi\", line 74, in jnius.populate_args\n",
      "  File \"jnius/jnius_utils.pxi\", line 193, in jnius.check_assignable_from_str\n",
      "TypeError: Invalid instance of 'java/lang/Float' passed for a 'java/lang/String'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23:11:14.395 [main] WARN org.terrier.structures.indexing.Indexer - skipping null document\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"jnius/jnius_proxy.pxi\", line 50, in jnius.PythonJavaClass.invoke\n",
      "  File \"jnius/jnius_proxy.pxi\", line 76, in jnius.PythonJavaClass._invoke\n",
      "  File \"/opt/homebrew/lib/python3.11/site-packages/pyterrier/index.py\", line 778, in next\n",
      "    lastdoc = self.convertFn(text, meta)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/lib/python3.11/site-packages/pyterrier/index.py\", line 687, in convertDoc\n",
      "    return TaggedDocument(StringReader(text_row), hashmap, Tokeniser.getTokeniser())\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"jnius/jnius_export_class.pxi\", line 269, in jnius.JavaClass.__init__\n",
      "  File \"jnius/jnius_export_class.pxi\", line 362, in jnius.JavaClass.call_constructor\n",
      "  File \"jnius/jnius_conversion.pxi\", line 74, in jnius.populate_args\n",
      "  File \"jnius/jnius_utils.pxi\", line 193, in jnius.check_assignable_from_str\n",
      "TypeError: Invalid instance of 'java/lang/Float' passed for a 'java/lang/String'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23:11:14.617 [main] WARN org.terrier.structures.indexing.Indexer - skipping null document\n",
      "23:11:14.660 [main] WARN org.terrier.structures.indexing.Indexer - skipping null document\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"jnius/jnius_proxy.pxi\", line 50, in jnius.PythonJavaClass.invoke\n",
      "  File \"jnius/jnius_proxy.pxi\", line 76, in jnius.PythonJavaClass._invoke\n",
      "  File \"/opt/homebrew/lib/python3.11/site-packages/pyterrier/index.py\", line 778, in next\n",
      "    lastdoc = self.convertFn(text, meta)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/lib/python3.11/site-packages/pyterrier/index.py\", line 687, in convertDoc\n",
      "    return TaggedDocument(StringReader(text_row), hashmap, Tokeniser.getTokeniser())\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"jnius/jnius_export_class.pxi\", line 269, in jnius.JavaClass.__init__\n",
      "  File \"jnius/jnius_export_class.pxi\", line 362, in jnius.JavaClass.call_constructor\n",
      "  File \"jnius/jnius_conversion.pxi\", line 74, in jnius.populate_args\n",
      "  File \"jnius/jnius_utils.pxi\", line 193, in jnius.check_assignable_from_str\n",
      "TypeError: Invalid instance of 'java/lang/Float' passed for a 'java/lang/String'\n",
      "Traceback (most recent call last):\n",
      "  File \"jnius/jnius_proxy.pxi\", line 50, in jnius.PythonJavaClass.invoke\n",
      "  File \"jnius/jnius_proxy.pxi\", line 76, in jnius.PythonJavaClass._invoke\n",
      "  File \"/opt/homebrew/lib/python3.11/site-packages/pyterrier/index.py\", line 778, in next\n",
      "    lastdoc = self.convertFn(text, meta)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/lib/python3.11/site-packages/pyterrier/index.py\", line 687, in convertDoc\n",
      "    return TaggedDocument(StringReader(text_row), hashmap, Tokeniser.getTokeniser())\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"jnius/jnius_export_class.pxi\", line 269, in jnius.JavaClass.__init__\n",
      "  File \"jnius/jnius_export_class.pxi\", line 362, in jnius.JavaClass.call_constructor\n",
      "  File \"jnius/jnius_conversion.pxi\", line 74, in jnius.populate_args\n",
      "  File \"jnius/jnius_utils.pxi\", line 193, in jnius.check_assignable_from_str\n",
      "TypeError: Invalid instance of 'java/lang/Float' passed for a 'java/lang/String'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23:11:14.836 [main] WARN org.terrier.structures.indexing.Indexer - skipping null document\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"jnius/jnius_proxy.pxi\", line 50, in jnius.PythonJavaClass.invoke\n",
      "  File \"jnius/jnius_proxy.pxi\", line 76, in jnius.PythonJavaClass._invoke\n",
      "  File \"/opt/homebrew/lib/python3.11/site-packages/pyterrier/index.py\", line 778, in next\n",
      "    lastdoc = self.convertFn(text, meta)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/lib/python3.11/site-packages/pyterrier/index.py\", line 687, in convertDoc\n",
      "    return TaggedDocument(StringReader(text_row), hashmap, Tokeniser.getTokeniser())\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"jnius/jnius_export_class.pxi\", line 269, in jnius.JavaClass.__init__\n",
      "  File \"jnius/jnius_export_class.pxi\", line 362, in jnius.JavaClass.call_constructor\n",
      "  File \"jnius/jnius_conversion.pxi\", line 74, in jnius.populate_args\n",
      "  File \"jnius/jnius_utils.pxi\", line 193, in jnius.check_assignable_from_str\n",
      "TypeError: Invalid instance of 'java/lang/Float' passed for a 'java/lang/String'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23:11:15.367 [main] WARN org.terrier.structures.indexing.Indexer - skipping null document\n",
      "23:11:15.467 [main] WARN org.terrier.structures.indexing.Indexer - skipping null document\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"jnius/jnius_proxy.pxi\", line 50, in jnius.PythonJavaClass.invoke\n",
      "  File \"jnius/jnius_proxy.pxi\", line 76, in jnius.PythonJavaClass._invoke\n",
      "  File \"/opt/homebrew/lib/python3.11/site-packages/pyterrier/index.py\", line 778, in next\n",
      "    lastdoc = self.convertFn(text, meta)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/lib/python3.11/site-packages/pyterrier/index.py\", line 687, in convertDoc\n",
      "    return TaggedDocument(StringReader(text_row), hashmap, Tokeniser.getTokeniser())\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"jnius/jnius_export_class.pxi\", line 269, in jnius.JavaClass.__init__\n",
      "  File \"jnius/jnius_export_class.pxi\", line 362, in jnius.JavaClass.call_constructor\n",
      "  File \"jnius/jnius_conversion.pxi\", line 74, in jnius.populate_args\n",
      "  File \"jnius/jnius_utils.pxi\", line 193, in jnius.check_assignable_from_str\n",
      "TypeError: Invalid instance of 'java/lang/Float' passed for a 'java/lang/String'\n",
      "Traceback (most recent call last):\n",
      "  File \"jnius/jnius_proxy.pxi\", line 50, in jnius.PythonJavaClass.invoke\n",
      "  File \"jnius/jnius_proxy.pxi\", line 76, in jnius.PythonJavaClass._invoke\n",
      "  File \"/opt/homebrew/lib/python3.11/site-packages/pyterrier/index.py\", line 778, in next\n",
      "    lastdoc = self.convertFn(text, meta)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/lib/python3.11/site-packages/pyterrier/index.py\", line 687, in convertDoc\n",
      "    return TaggedDocument(StringReader(text_row), hashmap, Tokeniser.getTokeniser())\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"jnius/jnius_export_class.pxi\", line 269, in jnius.JavaClass.__init__\n",
      "  File \"jnius/jnius_export_class.pxi\", line 362, in jnius.JavaClass.call_constructor\n",
      "  File \"jnius/jnius_conversion.pxi\", line 74, in jnius.populate_args\n",
      "  File \"jnius/jnius_utils.pxi\", line 193, in jnius.check_assignable_from_str\n",
      "TypeError: Invalid instance of 'java/lang/Float' passed for a 'java/lang/String'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23:11:16.835 [main] WARN org.terrier.structures.indexing.Indexer - skipping null document\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"jnius/jnius_proxy.pxi\", line 50, in jnius.PythonJavaClass.invoke\n",
      "  File \"jnius/jnius_proxy.pxi\", line 76, in jnius.PythonJavaClass._invoke\n",
      "  File \"/opt/homebrew/lib/python3.11/site-packages/pyterrier/index.py\", line 778, in next\n",
      "    lastdoc = self.convertFn(text, meta)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/lib/python3.11/site-packages/pyterrier/index.py\", line 687, in convertDoc\n",
      "    return TaggedDocument(StringReader(text_row), hashmap, Tokeniser.getTokeniser())\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"jnius/jnius_export_class.pxi\", line 269, in jnius.JavaClass.__init__\n",
      "  File \"jnius/jnius_export_class.pxi\", line 362, in jnius.JavaClass.call_constructor\n",
      "  File \"jnius/jnius_conversion.pxi\", line 74, in jnius.populate_args\n",
      "  File \"jnius/jnius_utils.pxi\", line 193, in jnius.check_assignable_from_str\n",
      "TypeError: Invalid instance of 'java/lang/Float' passed for a 'java/lang/String'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23:11:18.876 [main] WARN org.terrier.structures.indexing.Indexer - skipping null document\n",
      "23:11:18.951 [main] WARN org.terrier.structures.indexing.Indexer - skipping null document\n",
      "23:11:18.990 [main] WARN org.terrier.structures.indexing.Indexer - skipping null document\n",
      "23:11:18.996 [main] WARN org.terrier.structures.indexing.Indexer - skipping null document\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"jnius/jnius_proxy.pxi\", line 50, in jnius.PythonJavaClass.invoke\n",
      "  File \"jnius/jnius_proxy.pxi\", line 76, in jnius.PythonJavaClass._invoke\n",
      "  File \"/opt/homebrew/lib/python3.11/site-packages/pyterrier/index.py\", line 778, in next\n",
      "    lastdoc = self.convertFn(text, meta)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/lib/python3.11/site-packages/pyterrier/index.py\", line 687, in convertDoc\n",
      "    return TaggedDocument(StringReader(text_row), hashmap, Tokeniser.getTokeniser())\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"jnius/jnius_export_class.pxi\", line 269, in jnius.JavaClass.__init__\n",
      "  File \"jnius/jnius_export_class.pxi\", line 362, in jnius.JavaClass.call_constructor\n",
      "  File \"jnius/jnius_conversion.pxi\", line 74, in jnius.populate_args\n",
      "  File \"jnius/jnius_utils.pxi\", line 193, in jnius.check_assignable_from_str\n",
      "TypeError: Invalid instance of 'java/lang/Float' passed for a 'java/lang/String'\n",
      "Traceback (most recent call last):\n",
      "  File \"jnius/jnius_proxy.pxi\", line 50, in jnius.PythonJavaClass.invoke\n",
      "  File \"jnius/jnius_proxy.pxi\", line 76, in jnius.PythonJavaClass._invoke\n",
      "  File \"/opt/homebrew/lib/python3.11/site-packages/pyterrier/index.py\", line 778, in next\n",
      "    lastdoc = self.convertFn(text, meta)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/lib/python3.11/site-packages/pyterrier/index.py\", line 687, in convertDoc\n",
      "    return TaggedDocument(StringReader(text_row), hashmap, Tokeniser.getTokeniser())\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"jnius/jnius_export_class.pxi\", line 269, in jnius.JavaClass.__init__\n",
      "  File \"jnius/jnius_export_class.pxi\", line 362, in jnius.JavaClass.call_constructor\n",
      "  File \"jnius/jnius_conversion.pxi\", line 74, in jnius.populate_args\n",
      "  File \"jnius/jnius_utils.pxi\", line 193, in jnius.check_assignable_from_str\n",
      "TypeError: Invalid instance of 'java/lang/Float' passed for a 'java/lang/String'\n",
      "Traceback (most recent call last):\n",
      "  File \"jnius/jnius_proxy.pxi\", line 50, in jnius.PythonJavaClass.invoke\n",
      "  File \"jnius/jnius_proxy.pxi\", line 76, in jnius.PythonJavaClass._invoke\n",
      "  File \"/opt/homebrew/lib/python3.11/site-packages/pyterrier/index.py\", line 778, in next\n",
      "    lastdoc = self.convertFn(text, meta)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/lib/python3.11/site-packages/pyterrier/index.py\", line 687, in convertDoc\n",
      "    return TaggedDocument(StringReader(text_row), hashmap, Tokeniser.getTokeniser())\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"jnius/jnius_export_class.pxi\", line 269, in jnius.JavaClass.__init__\n",
      "  File \"jnius/jnius_export_class.pxi\", line 362, in jnius.JavaClass.call_constructor\n",
      "  File \"jnius/jnius_conversion.pxi\", line 74, in jnius.populate_args\n",
      "  File \"jnius/jnius_utils.pxi\", line 193, in jnius.check_assignable_from_str\n",
      "TypeError: Invalid instance of 'java/lang/Float' passed for a 'java/lang/String'\n",
      "Traceback (most recent call last):\n",
      "  File \"jnius/jnius_proxy.pxi\", line 50, in jnius.PythonJavaClass.invoke\n",
      "  File \"jnius/jnius_proxy.pxi\", line 76, in jnius.PythonJavaClass._invoke\n",
      "  File \"/opt/homebrew/lib/python3.11/site-packages/pyterrier/index.py\", line 778, in next\n",
      "    lastdoc = self.convertFn(text, meta)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/lib/python3.11/site-packages/pyterrier/index.py\", line 687, in convertDoc\n",
      "    return TaggedDocument(StringReader(text_row), hashmap, Tokeniser.getTokeniser())\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"jnius/jnius_export_class.pxi\", line 269, in jnius.JavaClass.__init__\n",
      "  File \"jnius/jnius_export_class.pxi\", line 362, in jnius.JavaClass.call_constructor\n",
      "  File \"jnius/jnius_conversion.pxi\", line 74, in jnius.populate_args\n",
      "  File \"jnius/jnius_utils.pxi\", line 193, in jnius.check_assignable_from_str\n",
      "TypeError: Invalid instance of 'java/lang/Float' passed for a 'java/lang/String'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23:11:19.576 [main] WARN org.terrier.structures.indexing.Indexer - skipping null document\n",
      "23:11:19.688 [main] WARN org.terrier.structures.indexing.Indexer - skipping null document\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"jnius/jnius_proxy.pxi\", line 50, in jnius.PythonJavaClass.invoke\n",
      "  File \"jnius/jnius_proxy.pxi\", line 76, in jnius.PythonJavaClass._invoke\n",
      "  File \"/opt/homebrew/lib/python3.11/site-packages/pyterrier/index.py\", line 778, in next\n",
      "    lastdoc = self.convertFn(text, meta)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/lib/python3.11/site-packages/pyterrier/index.py\", line 687, in convertDoc\n",
      "    return TaggedDocument(StringReader(text_row), hashmap, Tokeniser.getTokeniser())\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"jnius/jnius_export_class.pxi\", line 269, in jnius.JavaClass.__init__\n",
      "  File \"jnius/jnius_export_class.pxi\", line 362, in jnius.JavaClass.call_constructor\n",
      "  File \"jnius/jnius_conversion.pxi\", line 74, in jnius.populate_args\n",
      "  File \"jnius/jnius_utils.pxi\", line 193, in jnius.check_assignable_from_str\n",
      "TypeError: Invalid instance of 'java/lang/Float' passed for a 'java/lang/String'\n",
      "Traceback (most recent call last):\n",
      "  File \"jnius/jnius_proxy.pxi\", line 50, in jnius.PythonJavaClass.invoke\n",
      "  File \"jnius/jnius_proxy.pxi\", line 76, in jnius.PythonJavaClass._invoke\n",
      "  File \"/opt/homebrew/lib/python3.11/site-packages/pyterrier/index.py\", line 778, in next\n",
      "    lastdoc = self.convertFn(text, meta)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/lib/python3.11/site-packages/pyterrier/index.py\", line 687, in convertDoc\n",
      "    return TaggedDocument(StringReader(text_row), hashmap, Tokeniser.getTokeniser())\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"jnius/jnius_export_class.pxi\", line 269, in jnius.JavaClass.__init__\n",
      "  File \"jnius/jnius_export_class.pxi\", line 362, in jnius.JavaClass.call_constructor\n",
      "  File \"jnius/jnius_conversion.pxi\", line 74, in jnius.populate_args\n",
      "  File \"jnius/jnius_utils.pxi\", line 193, in jnius.check_assignable_from_str\n",
      "TypeError: Invalid instance of 'java/lang/Float' passed for a 'java/lang/String'\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T22:24:52.901623Z",
     "start_time": "2024-05-21T22:24:52.883962Z"
    }
   },
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(f'./Data/all.csv')[[\"titre\",\"contenu\"]]\n",
    "df = df.fillna('missing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sqd(q, topic_pagerank, D, terms_indexes, index):\n",
    "    # Using a text index, recuperer les docs qui contiennent q\n",
    "    br = pt.BatchRetrieve(index, wmodel=\"CoordinateMatch\") # CoordinateMatch renvoie 1 si le terme est dans le doc, 0 sinon\n",
    "    docs_retrieved = br.search(q) # Renvoie le nombre de termes de la query qui sont dans chaque doc\n",
    "    docs_query = docs_retrieved[docs_retrieved[\"score\"] == len(q.split())]  # Garder uniquement les docs qui contiennent tous les termes de la query\n",
    "    docs_index = docs_query[\"docno\"].values\n",
    "    \n",
    "    # calculer P(cj|q)\n",
    "    probas_c_q = np.zeros((16))\n",
    "    p_c = 1/16 # \"The quantity P(cj) is not as straightforward. We chose to make it uniform\"\n",
    "\n",
    "    for i,topic in enumerate(Topic_names):\n",
    "        indexes = np.in1d(terms_indexes[topic], np.array(q)).nonzero()[0]\n",
    "\n",
    "        if len(indexes)==0:\n",
    "            probas_c_q[i] = 0\n",
    "        else:\n",
    "            p_q_c = D[topic].tocsc()[indexes]/D[topic].tocsc().sum()\n",
    "            p_q_c = p_q_c.toarray()\n",
    "            probas_c_q[i] = p_c * np.product(p_q_c)\n",
    "            \n",
    "    df = pd.read_csv(f'./Data/all.csv')\n",
    "    df.columns = [\"id\", \"titre\", \"contenu\", \"liens\"]\n",
    "            \n",
    "    docs = df[df[\"titre\"].isin(docs_index)]\n",
    "    \n",
    "    # print(docs)\n",
    "    \n",
    "    docs[\"score\"] = docs.apply(lambda x: sum([probas_c_q[Topic_names.index(topic)]*topic_pagerank[topic][x[\"id\"]] for topic in Topic_names]), axis=1)\n",
    "    \n",
    "    # print(docs)\n",
    "\n",
    "    # garder les 3 meilleurs cj\n",
    "    # sorted_indices = np.argsort(-probas_c_q)\n",
    "    # \n",
    "    # top_indices = sorted_indices[:3]\n",
    "    # top_values = probas_c_q[top_indices]\n",
    "    # \n",
    "    # # somme des P(cj|q) * rank j\n",
    "    # res = 0\n",
    "    # for i in range(3):\n",
    "    #     res+= top_values[i]*topic_pagerank[Topic_names[top_indices[i]]]\n",
    "    #return np.sum(top_values*topic_pagerank[top_indices],axis=1)\n",
    "\n",
    "\n",
    "    # meilleurs documents :\n",
    "\n",
    "    # doc qui ont les mots de la query sans pyterrier\n",
    "    # df['text_lower'] = df['contenu'].str.lower() # Convert the text column to lowercase for case-insensitive search\n",
    "    # query_words = q.lower().split() # Split the query into individual words\n",
    "    # mask = df['text_lower'].str.contains('|'.join(query_words)) # boolean mask to filter rows containing any of the query words\n",
    "    # doc_indices = df.index[mask] # indices des docs qui ont la query\n",
    "    \n",
    "    #res[doc_indices]+=100 # on leur donne plus de valeur pour qu'ils sortent d'abord\n",
    "\n",
    "    #sorted_docs_index = np.argsort(-res)\n",
    "    # best_docs = pd.read_csv(f'./Data/all.csv')[\"titre\"].values[sorted_docs_index]\n",
    "\n",
    "    best_docs = docs.sort_values(by=\"score\", ascending=False)[\"titre\"]\n",
    "    return best_docs"
   ],
   "outputs": [],
   "execution_count": 66
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T22:24:55.361183Z",
     "start_time": "2024-05-21T22:24:53.298783Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# query = \"affirmative action\"\n",
    "# print(sqd(query, topic_pagerank, D, terms_indexes, index)[:5])"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8        Expressive therapies\n",
      "11499        Apple and unions\n",
      "12423           Shadow family\n",
      "12384           Public sphere\n",
      "12357    Gender mainstreaming\n",
      "Name: titre, dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/k2/pykvndls4t122_sjk529gvvc0000gn/T/ipykernel_15530/4254619777.py:29: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  docs[\"score\"] = docs.apply(lambda x: sum([probas_c_q[Topic_names.index(topic)]*topic_pagerank[topic][x[\"id\"]] for topic in Topic_names]), axis=1)\n"
     ]
    }
   ],
   "execution_count": 67
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T22:31:47.585703Z",
     "start_time": "2024-05-21T22:31:47.578915Z"
    }
   },
   "source": [
    "def score_normal_pagerank(q,normal_pagerank):\n",
    "    \n",
    "    # # doc qui ont les mots de la query (sans pyterrier)\n",
    "    # df['text_lower'] = df['contenu'].str.lower() # Convert the text column to lowercase for case-insensitive search\n",
    "    # query_words = q.lower().split() # Split the query into individual words\n",
    "    # mask = df['text_lower'].str.contains('|'.join(query_words)) # boolean mask to filter rows containing any of the query words\n",
    "    # doc_indices = df.index[mask] # indices des docs qui ont la query\n",
    "    # normal_pagerank[doc_indices] +=100\n",
    "    # \n",
    "    # sorted_docs_index = np.argsort(-normal_pagerank)\n",
    "    # best_docs = pd.read_csv(f'./Data/all.csv')[\"titre\"].values[sorted_docs_index]\n",
    "    \n",
    "    # Using a text index, recuperer les docs qui contiennent q\n",
    "    br = pt.BatchRetrieve(index, wmodel=\"CoordinateMatch\") # CoordinateMatch renvoie 1 si le terme est dans le doc, 0 sinon\n",
    "    docs_retrieved = br.search(q) # Renvoie le nombre de termes de la query qui sont dans chaque doc\n",
    "    docs_query = docs_retrieved[docs_retrieved[\"score\"] == len(q.split())]  # Garder uniquement les docs qui contiennent tous les termes de la query\n",
    "    sorted_index = docs_query[\"docno\"].values\n",
    "    \n",
    "    df = pd.read_csv(f'./Data/all.csv')\n",
    "    df.columns = [\"id\", \"titre\", \"contenu\", \"liens\"]\n",
    "    \n",
    "    docs = df[df[\"titre\"].isin(sorted_index)]\n",
    "    docs[\"score\"] = normal_pagerank[docs[\"id\"].values]\n",
    "    \n",
    "    best_docs = docs.sort_values(by=\"score\", ascending=False)[\"titre\"]\n",
    "    \n",
    "    return best_docs\n"
   ],
   "outputs": [],
   "execution_count": 82
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimental results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarity ranking (4.1)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T22:25:03.316252Z",
     "start_time": "2024-05-21T22:25:03.302687Z"
    }
   },
   "source": [
    "def OSim(t1, t2, n=20):\n",
    "    # t1, t2 listes\n",
    "    return len(set(t1[:n])&set(t2[:n]))/n"
   ],
   "outputs": [],
   "execution_count": 69
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T22:25:03.747276Z",
     "start_time": "2024-05-21T22:25:03.741410Z"
    }
   },
   "source": [
    "import itertools\n",
    "\n",
    "def KSim(t1, t2):\n",
    "    # t1, t2 listes\n",
    "    U = set(t1)|set(t2)\n",
    "    delta1 = U - set(t1)\n",
    "    delta2 = U - set(t2)\n",
    "\n",
    "    t1_prime = list(t1)+list(delta1)\n",
    "    t2_prime = list(t2)+list(delta2)\n",
    "\n",
    "    sim = 0\n",
    "    for u,v in list(itertools.permutations(U, 2)):\n",
    "        if np.sign(t1_prime.index(u)-t1_prime.index(v))==np.sign(t2_prime.index(u)-t2_prime.index(v)):\n",
    "            sim+=1\n",
    "    return sim/(len(U)*(len(U)-1))\n"
   ],
   "outputs": [],
   "execution_count": 70
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tests on queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T22:25:05.163632Z",
     "start_time": "2024-05-21T22:25:05.144329Z"
    }
   },
   "source": [
    "queries = [ \"affirmative action\", \"alcoholism\", \"amusement parks\", \n",
    "            \"architecture\", \"bicycling\", \"blues\", \"cheese\", \n",
    "            \"citrus groves\", \"classical guitar\", \"computer vision\", \n",
    "            \"cruises\", \"death valley\", \"field hockey\", \n",
    "            \"gardening\", \"graphic design\", \"gulf war\", \n",
    "            \"hiv\", \"java\", \"lipari\",\n",
    "            \"lyme disease\", \"mutual funds\", \"national parks\", \n",
    "            \"parallel architecture\", \"recycling cans\", \"rock climbing\", \n",
    "            \"san francisco\", \"shakespeare\", \"stamp collecting\", \n",
    "            \"sushi\", \"table tennis\", \"telecommuting\", \n",
    "            \"vintage cars\", \"volcano\", \"zen buddhism\", \"zener\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "with open(\"terms_indexes.pkl\", \"rb\") as f:\n",
    "    terms_indexes = pickle.load(f)\n",
    "with open(\"D.pkl\", \"rb\") as f:\n",
    "    D = pickle.load(f)\n",
    "    for topic in Topic_names:\n",
    "        D[topic] = np.reshape(D[topic],(D[topic].shape[1],1))\n",
    "with open(\"topic_pagerank.pkl\", \"rb\") as f:\n",
    "    topic_pagerank = pickle.load(f)\n",
    "    \n",
    "index = pt.IndexFactory.of(\"./pd_index\")"
   ],
   "outputs": [],
   "execution_count": 72
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T22:25:16.089366Z",
     "start_time": "2024-05-21T22:25:07.666537Z"
    }
   },
   "source": [
    "for q in queries[:5]:\n",
    "    print(q)\n",
    "    print(sqd(q, topic_pagerank, D, terms_indexes, index)[:5])\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "affirmative action\n",
      "['Art world' 'Cowles Art School' 'Cirebonese mask dance'\n",
      " 'Silent Energy: New Art from China'\n",
      " 'Public art of the Washington State Ferries system']\n",
      "alcoholism\n",
      "['Joel-Peter Witkin' 'Gandharvanpattu' 'Nowhereisland'\n",
      " 'New Theatre Quarterly' 'Art']\n",
      "amusement parks\n",
      "['Jules Trobaugh' 'Society of Modern Women Artists' 'Tortillon'\n",
      " 'Mediology' 'Suspension of disbelief']\n",
      "architecture\n",
      "['Leonardo (journal)' 'Narration' 'Screenplay' 'Narrative hook' 'Lath art']\n",
      "bicycling\n",
      "['Maternal mortality in fiction' 'Gerald Nordland' 'Film à clef'\n",
      " 'After (art)' 'Corded quilting']\n"
     ]
    }
   ],
   "source": [
    "for q in queries[:5]:\n",
    "    print(q)\n",
    "    print(sqd(q, topic_pagerank, D, terms_indexes, None)[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"normal_pagerank.pkl\", \"rb\") as f:\n",
    "        normal_pagerank = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "affirmative action\n",
      "['Film à clef' 'Corded quilting' 'Ristra' 'Maternal mortality in fiction'\n",
      " 'Gerald Nordland']\n",
      "alcoholism\n",
      "['Film à clef' 'Corded quilting' 'Ristra' 'Nowhereisland'\n",
      " 'Maternal mortality in fiction']\n",
      "amusement parks\n",
      "['Film à clef' 'Corded quilting' 'Ristra' 'Maternal mortality in fiction'\n",
      " 'Nowhereisland']\n",
      "architecture\n",
      "['Film à clef' 'Maternal mortality in fiction' 'Corded quilting'\n",
      " 'Gerald Nordland' 'Ristra']\n",
      "bicycling\n",
      "['Film à clef' 'Maternal mortality in fiction' 'Corded quilting'\n",
      " 'Gerald Nordland' 'Ristra']\n"
     ]
    }
   ],
   "source": [
    "for q in queries[:5]:\n",
    "    print(q)\n",
    "    print(score_normal_pagerank(q, normal_pagerank)[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "affirmative action\n",
      "['Artisan' 'Creative class' 'Andy Warhol' 'Self-fulfilling prophecy'\n",
      " 'Performance art']\n",
      "alcoholism\n",
      "['Nowhereisland' 'New Theatre Quarterly' 'Gandharvanpattu' 'Artisan'\n",
      " 'Creative class']\n",
      "amusement parks\n",
      "['Pseudohistory' 'Vignette (literature)' 'Maternal mortality in fiction'\n",
      " 'After (art)' 'Tortillon']\n",
      "architecture\n",
      "['Maternal mortality in fiction' 'Gerald Nordland' 'Screenplay'\n",
      " 'Pseudohistory' 'Leonardo (journal)']\n",
      "bicycling\n",
      "['Maternal mortality in fiction' 'Gerald Nordland' 'Screenplay'\n",
      " 'Pseudohistory' 'Leonardo (journal)']\n"
     ]
    }
   ],
   "source": [
    "for q in queries[:5]:\n",
    "    print(q)\n",
    "    print(score_normal_pagerank(q, topic_pagerank['business'])[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "affirmative action\n",
      "['Film à clef' 'Corded quilting' 'Ristra' 'Maternal mortality in fiction'\n",
      " 'After (art)']\n",
      "alcoholism\n",
      "['Film à clef' 'Corded quilting' 'Ristra' 'Maternal mortality in fiction'\n",
      " 'After (art)']\n",
      "amusement parks\n",
      "['Maternal mortality in fiction' 'After (art)' 'Film à clef'\n",
      " 'Corded quilting' 'Plot hole']\n",
      "architecture\n",
      "['Maternal mortality in fiction' 'Gerald Nordland' 'After (art)'\n",
      " 'Film à clef' 'Corded quilting']\n",
      "bicycling\n",
      "['Maternal mortality in fiction' 'Gerald Nordland' 'After (art)'\n",
      " 'Film à clef' 'Corded quilting']\n"
     ]
    }
   ],
   "source": [
    "for q in queries[:5]:\n",
    "    print(q)\n",
    "    print(score_normal_pagerank(q, topic_pagerank['sports'])[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarité entre les ranking de pagerank normal et de pagerank avec topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "affirmative action \tOSim: 0.05\n",
      "alcoholism \tOSim: 0.05\n",
      "amusement parks \tOSim: 0.2\n",
      "architecture \tOSim: 0.15\n",
      "bicycling \tOSim: 0.75\n",
      "blues \tOSim: 0.3\n",
      "cheese \tOSim: 0.25\n",
      "citrus groves \tOSim: 0.05\n",
      "classical guitar \tOSim: 0.15\n",
      "computer vision \tOSim: 0.0\n",
      "cruises \tOSim: 0.0\n",
      "death valley \tOSim: 0.45\n",
      "field hockey \tOSim: 0.1\n",
      "gardening \tOSim: 0.0\n",
      "graphic design \tOSim: 0.0\n",
      "gulf war \tOSim: 0.2\n",
      "hiv \tOSim: 0.1\n",
      "java \tOSim: 0.0\n",
      "lipari \tOSim: 0.0\n",
      "lyme disease \tOSim: 0.05\n",
      "mutual funds \tOSim: 0.0\n",
      "national parks \tOSim: 0.0\n",
      "parallel architecture \tOSim: 0.15\n",
      "recycling cans \tOSim: 0.0\n",
      "rock climbing \tOSim: 0.2\n",
      "san francisco \tOSim: 0.1\n",
      "shakespeare \tOSim: 0.2\n",
      "stamp collecting \tOSim: 0.1\n",
      "sushi \tOSim: 0.05\n",
      "table tennis \tOSim: 0.0\n",
      "telecommuting \tOSim: 0.0\n",
      "vintage cars \tOSim: 0.15\n",
      "volcano \tOSim: 0.0\n",
      "zen buddhism \tOSim: 0.0\n",
      "zener \tOSim: 0.0\n"
     ]
    }
   ],
   "source": [
    "for q in queries:\n",
    "    normal = score_normal_pagerank(q, normal_pagerank)\n",
    "    topic = sqd(q, topic_pagerank, D, terms_indexes, None)\n",
    "    print(q,\"\\tOSim:\",OSim(normal, topic, n=20))\n",
    "    #print(\"KSim,\",KSim(normal[:20], topic[:20]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "affirmative action \tKSim, 0.25384615384615383\n",
      "alcoholism \tKSim, 0.5954415954415955\n",
      "amusement parks \tKSim, 0.27692307692307694\n",
      "architecture \tKSim, 0.22435897435897437\n",
      "bicycling \tKSim, 0.30229419703103916\n",
      "blues \tKSim, 0.38257575757575757\n",
      "cheese \tKSim, 0.2807807807807808\n",
      "citrus groves \tKSim, 0.16984126984126985\n",
      "classical guitar \tKSim, 0.2822822822822823\n",
      "computer vision \tKSim, 0.2076923076923077\n",
      "cruises \tKSim, 0.2512820512820513\n",
      "death valley \tKSim, 0.22948717948717948\n",
      "field hockey \tKSim, 0.2739541160593792\n",
      "gardening \tKSim, 0.30014224751066854\n",
      "graphic design \tKSim, 0.28205128205128205\n",
      "gulf war \tKSim, 0.2739541160593792\n",
      "hiv \tKSim, 0.26720647773279355\n",
      "java \tKSim, 0.267425320056899\n",
      "lipari \tKSim, 0.24426450742240216\n",
      "lyme disease \tKSim, 0.26180836707152494\n",
      "mutual funds \tKSim, 0.24039829302987198\n",
      "national parks \tKSim, 0.25889046941678523\n",
      "parallel architecture \tKSim, 0.283072546230441\n",
      "recycling cans \tKSim, 0.25775978407557354\n",
      "rock climbing \tKSim, 0.2777777777777778\n",
      "san francisco \tKSim, 0.28609986504723345\n",
      "shakespeare \tKSim, 0.2222222222222222\n",
      "stamp collecting \tKSim, 0.2233285917496444\n",
      "sushi \tKSim, 0.29160739687055476\n",
      "table tennis \tKSim, 0.2483130904183536\n",
      "telecommuting \tKSim, 0.26450742240215924\n",
      "vintage cars \tKSim, 0.3476190476190476\n",
      "volcano \tKSim, 0.28733997155049784\n",
      "zen buddhism \tKSim, 0.2532005689900427\n",
      "zener \tKSim, 0.24324324324324326\n"
     ]
    }
   ],
   "source": [
    "for q in queries:\n",
    "    normal = score_normal_pagerank(q, normal_pagerank)\n",
    "    topic = sqd(q, topic_pagerank, D, terms_indexes, None)\n",
    "    print(q,\"\\tKSim,\",KSim(normal[:20], topic[:20]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparaison des vecteurs PageRank (table 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "pageranks = topic_pagerank.copy()\n",
    "pageranks[\"normal\"] = normal_pagerank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normal arts\n",
      "normal games\n"
     ]
    }
   ],
   "source": [
    "table2={}\n",
    "for topic_1,topic_2 in list(itertools.permutations([\"normal\"]+Topic_names, 2)):\n",
    "    print(topic_1,topic_2)\n",
    "    sim = 0\n",
    "    for q in queries:\n",
    "        indexation1 = score_normal_pagerank(q, pageranks[topic_1])\n",
    "        indexation2 = score_normal_pagerank(q, pageranks[topic_2])\n",
    "        sim += KSim(indexation1[:20], indexation2[:20])\n",
    "    sim /= len(queries)\n",
    "    table2 = {f\"{topic_1}/{topic_2}\": sim}\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
