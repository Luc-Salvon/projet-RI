{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A faire : \n",
    "- indexer les articles (et indice inversé)\n",
    "- changer les noms des fichiers json pour qu'ils soient appelés par leur titre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install wikipedia-api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wikipediaapi\n",
    "\n",
    "\n",
    "wiki_wiki = wikipediaapi.Wikipedia(\n",
    "    user_agent='MyProjectName',\n",
    "        language='en',\n",
    "        extract_format=wikipediaapi.ExtractFormat.WIKI\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Get data for each category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import shuffle\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def url_categorymembers(categorymembers,list_pages=[], level=0, max_level=3, i=0):\n",
    "    pages_cat = list(categorymembers.values()) #toutes les pages de la catégorie\n",
    "\n",
    "    shuffle(pages_cat)\n",
    "    print(i)\n",
    "\n",
    "    for c in pages_cat:\n",
    "        if i==1000: #1000 pages par catégorie\n",
    "            break\n",
    "\n",
    "        if c.title[:6]!=\"Portal\" and c.title[:5]!=\"File:\" and c.title[:8]!=\"Category\": # vérifie que c'est bien un article\n",
    "            i+=1\n",
    "            #list_urls.append(c.fullurl)\n",
    "            #list_pages.append(c)\n",
    "            #list_pages.append(c.title)\n",
    "            d = {}\n",
    "            d[\"titre\"] = c.title\n",
    "            d[\"contenu\"] = c.text\n",
    "            d[\"liens\"] = list(c.links.keys())\n",
    "            list_pages.append(d)\n",
    "\n",
    "        if c.ns == wikipediaapi.Namespace.CATEGORY and level < max_level: # descend d'un niveau\n",
    "            i = url_categorymembers(c.categorymembers, list_pages, level=level + 1, max_level=max_level, i=i)[1]\n",
    "\n",
    "    return list_pages, i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_arts = wiki_wiki.page(\"Category:The arts\") # Arts\n",
    "arts, _ = url_categorymembers(cat_arts.categorymembers)\n",
    "pd.DataFrame(arts).to_csv('arts.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_games = wiki_wiki.page(\"Category:Games\") # Games\n",
    "games, _= url_categorymembers(cat_games.categorymembers)\n",
    "\n",
    "pd.DataFrame(games).to_csv('games.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_youth = wiki_wiki.page(\"Category:Youth\") # Kids and Teens (not exact)\n",
    "youth, _ = url_categorymembers(cat_youth.categorymembers)\n",
    "\n",
    "pd.DataFrame(youth).to_csv('youth.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_reference = wiki_wiki.page(\"Category:Reference\") # Reference\n",
    "reference, _ = url_categorymembers(cat_reference.categorymembers)\n",
    "\n",
    "pd.DataFrame(reference).to_csv('reference.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_shopping = wiki_wiki.page(\"Category:Retailing\") # Shopping\n",
    "shopping, _ = url_categorymembers(cat_shopping.categorymembers)\n",
    "\n",
    "pd.DataFrame(shopping).to_csv('shopping.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_business = wiki_wiki.page(\"Category:Business\") # Business\n",
    "business, _ = url_categorymembers(cat_business.categorymembers)\n",
    "\n",
    "pd.DataFrame(business).to_csv('business.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_health = wiki_wiki.page(\"Category:Health\") # Health\n",
    "health, _ = url_categorymembers(cat_health.categorymembers)\n",
    "\n",
    "pd.DataFrame(health).to_csv('health.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_news = wiki_wiki.page(\"Category:News\") # News\n",
    "news, _ = url_categorymembers(cat_news.categorymembers)\n",
    "\n",
    "pd.DataFrame(news).to_csv('news.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_geography = wiki_wiki.page(\"Category:Geography\") # Regional (not exact)\n",
    "geography, _ = url_categorymembers(cat_geography.categorymembers)\n",
    "\n",
    "pd.DataFrame(geography).to_csv('geography.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_society = wiki_wiki.page(\"Category:Society\") # Society\n",
    "society, _ = url_categorymembers(cat_society.categorymembers)\n",
    "\n",
    "pd.DataFrame(society).to_csv('society.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_computers = wiki_wiki.page(\"Category:Computers\") # Computers\n",
    "computers, _ = url_categorymembers(cat_computers.categorymembers)\n",
    "\n",
    "df = pd.DataFrame(computers)\n",
    "df.to_csv('computers.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_home = wiki_wiki.page(\"Category:Home\") # Home\n",
    "home, _ = url_categorymembers(cat_home.categorymembers)\n",
    "\n",
    "df = pd.DataFrame(home)\n",
    "df.to_csv('home.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_recreation = wiki_wiki.page(\"Category:Recreation\") # Recreation\n",
    "recreation, _ = url_categorymembers(cat_recreation.categorymembers)\n",
    "\n",
    "pd.DataFrame(recreation).to_csv('recreation.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_science = wiki_wiki.page(\"Category:Science\") # Science\n",
    "science, _ = url_categorymembers(cat_science.categorymembers)\n",
    "\n",
    "pd.DataFrame(science).to_csv('science.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_sports = wiki_wiki.page(\"Category:Sports\") # Sports\n",
    "sports, _ = url_categorymembers(cat_sports.categorymembers)\n",
    "\n",
    "pd.DataFrame(sports).to_csv('sports.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_world = wiki_wiki.page(\"Category:World\") # World\n",
    "world, _ = url_categorymembers(cat_world.categorymembers)\n",
    "\n",
    "pd.DataFrame(world).to_csv('world.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pas exécuté\n",
    "\n",
    "topics = {\n",
    "    \"Arts\": arts,\n",
    "    \"Games\": games,\n",
    "    \"Youth\": youth,\n",
    "    \"Reference\": reference, \n",
    "    \"Shopping\": shopping, \n",
    "    \"Business\": business, \n",
    "    \"Health\": health, \n",
    "    \"News\": news,\n",
    "    \"Geography\": geography,\n",
    "    \"Society\": society,\n",
    "    \"Computers\": computers,\n",
    "    \"Home\": home,\n",
    "    \"Recreation\": recreation,\n",
    "    \"Science\": science,\n",
    "    \"Sports\": sports,\n",
    "    \"World\": world\n",
    "}\n",
    "\n",
    "\n",
    "import pickle \n",
    "\n",
    "with open('titres_articles_categories.pkl', 'wb') as f:\n",
    "    pickle.dump(topics, f)\n",
    "        \n",
    "#with open('titres_articles_categories.pkl', 'rb') as f:\n",
    "    #topics = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Indexation des articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyterrier as pt\n",
    "if not pt.started():\n",
    "    pt.init()\n",
    "\n",
    "df = pd.DataFrame(liste_documents, columns=[\"docno\", \"text\"])\n",
    "\n",
    "index_path = \"./index\"\n",
    "\n",
    "indexer = pt.IterDictIndexer(index_path)\n",
    "index_ref = indexer.index(df[\"text\"], df[\"docno\"])\n",
    "\n",
    "index = pt.IndexFactory.of(index_ref)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute Page Rank vectors (3.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrices d'adjacence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construction de la matrice sans utiliser les fichiers :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "# build the adjacency matrix\n",
    "def build_adjacency_matrix(df):\n",
    "    import ast\n",
    "    num_pages = df.shape[0]\n",
    "    adjacency_matrix = np.zeros((num_pages, num_pages))\n",
    "    df[\"liens\"] = df[\"liens\"].apply(ast.literal_eval)\n",
    "    \n",
    "    for index, row in df.iterrows():\n",
    "        for link in row['liens']:\n",
    "            if link in df['titre'].values:\n",
    "                j = np.where(df['titre'] == link )[0]\n",
    "                adjacency_matrix[index][j] += 1\n",
    "\n",
    "    return adjacency_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrice_arts = build_adjacency_matrix(pd.read_csv('./Data/arts.csv'))\n",
    "np.save('./Matrices/matrice_arts.npy', matrice_arts)\n",
    "\n",
    "matrice_games = build_adjacency_matrix(pd.read_csv('./Data/games.csv'))\n",
    "np.save('./Matrices/matrice_games.npy', matrice_games)\n",
    "\n",
    "matrice_youth = build_adjacency_matrix(pd.read_csv('./Data/youth.csv'))\n",
    "np.save('./Matrices/matrice_youth.npy', matrice_youth)\n",
    "\n",
    "matrice_reference = build_adjacency_matrix(pd.read_csv('./Data/reference.csv'))\n",
    "np.save('./Matrices/matrice_reference.npy', matrice_reference)\n",
    "\n",
    "matrice_shopping = build_adjacency_matrix(pd.read_csv('./Data/shopping.csv'))\n",
    "np.save('./Matrices/matrice_shopping.npy', matrice_shopping)\n",
    "\n",
    "matrice_business = build_adjacency_matrix(pd.read_csv('./Data/business.csv'))\n",
    "np.save('./Matrices/matrice_business.npy', matrice_business)\n",
    "\n",
    "matrice_health = build_adjacency_matrix(pd.read_csv('./Data/health.csv'))\n",
    "np.save('./Matrices/matrice_health.npy', matrice_health)\n",
    "\n",
    "matrice_news = build_adjacency_matrix(pd.read_csv('./Data/news.csv'))\n",
    "np.save('./Matrices/matrice_news.npy', matrice_news)\n",
    "\n",
    "matrice_geography = build_adjacency_matrix(pd.read_csv('./Data/geography.csv'))\n",
    "np.save('./Matrices/matrice_geography.npy', matrice_geography)\n",
    "\n",
    "matrice_society = build_adjacency_matrix(pd.read_csv('./Data/society.csv'))\n",
    "np.save('./Matrices/matrice_society.npy', matrice_society)\n",
    "\n",
    "matrice_computers = build_adjacency_matrix(pd.read_csv('./Data/computers.csv'))\n",
    "np.save('./Matrices/matrice_computers.npy', matrice_computers)\n",
    "\n",
    "matrice_home = build_adjacency_matrix(pd.read_csv('./Data/home.csv'))\n",
    "np.save('./Matrices/matrice_home.npy', matrice_home)\n",
    "\n",
    "matrice_recreation = build_adjacency_matrix(pd.read_csv('./Data/recreation.csv'))\n",
    "np.save('./Matrices/matrice_recreation.npy', matrice_recreation)\n",
    "\n",
    "matrice_science = build_adjacency_matrix(pd.read_csv('./Data/science.csv'))\n",
    "np.save('./Matrices/matrice_science.npy', matrice_science)\n",
    "\n",
    "matrice_sports = build_adjacency_matrix(pd.read_csv('./Data/sports.csv'))\n",
    "np.save('./Matrices/matrice_sports.npy', matrice_sports)\n",
    "\n",
    "matrice_world = build_adjacency_matrix(pd.read_csv('./Data/world.csv'))\n",
    "np.save('./Matrices/matrice_world.npy', matrice_world)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construction de la matrice en utilisant les fichiers :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "fichier_graphe = pandas.read_csv('enwiki.wikilink_graph.2018-03-01.csv')\n",
    "\n",
    "# build the adjacency matrix\n",
    "def build_adjacency_matrix(topic_articles, inversed_index_articles):\n",
    "    num_pages = len(topic_articles)\n",
    "    adjacency_matrix = np.zeros((num_pages, num_pages))\n",
    "\n",
    "    for i,titre in enumerate(topic_articles):\n",
    "        #links = page_links(pages[i])\n",
    "        links = fichier_graphe[fichier_graphe['page_title_from']==titre]\n",
    "        for link in links:\n",
    "            if link in topic_articles:\n",
    "                j = topic_articles.index(link)\n",
    "                adjacency_matrix[i][j] = 1\n",
    "\n",
    "    return adjacency_matrix/sum(adjacency_matrix)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calcul des vecteurs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute PageRank scores\n",
    "def compute_pagerank(M, alpha=0.25):\n",
    "    # M = adjacency_matrix\n",
    "    # damping_factor = 1-alpha\n",
    "    N = M.shape[0]\n",
    "    teleportation_matrix = np.ones((N, N)) / N # pour page_rank normal, pas topic_sensitive\n",
    "\n",
    "    M_prime = (1-alpha) * M + alpha * teleportation_matrix\n",
    "\n",
    "    rank = np.ones(N) / N\n",
    "    old_rank = np.zeros(N)\n",
    "\n",
    "    epsilon = 1.0e-6\n",
    "    max_iterations = 100\n",
    "    iterations = 0\n",
    "\n",
    "    while np.sum(np.abs(rank - old_rank)) > epsilon and iterations < max_iterations:\n",
    "        old_rank = rank.copy()\n",
    "        rank = np.dot(M_prime, rank)\n",
    "        iterations += 1\n",
    "\n",
    "    return rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: arts\n",
      "[3.57792215e+139 9.82148846e+137 3.87579877e+138 1.16556414e+138\n",
      " 1.53235456e+139]\n",
      "Topic: games\n",
      "[7.36900746e+191 7.36900746e+191 8.51526595e+191 7.36900746e+191\n",
      " 7.36900746e+191]\n",
      "Topic: youth\n",
      "[2.17940507e+116 9.09707512e+113 8.86972372e+113 1.05170414e+114\n",
      " 8.23375819e+113]\n",
      "Topic: reference\n",
      "[2.65095748e+81 1.27821056e+82 4.34150774e+81 7.52236481e+80\n",
      " 8.61124918e+79]\n",
      "Topic: shopping\n",
      "[1.90523685e+108 1.72122161e+109 1.72122161e+109 1.57960848e+110\n",
      " 2.46647545e+109]\n",
      "Topic: business\n",
      "[1.06911538e+164 5.65099706e+162 1.52264262e+162 1.10800386e+164\n",
      " 3.26720829e+165]\n",
      "Topic: health\n",
      "[3.01604329e+148 2.61682936e+149 1.64402604e+150 2.38024058e+150\n",
      " 1.58167267e+150]\n",
      "Topic: news\n",
      "[1.89133523e+167 5.95992097e+169 4.82703208e+168 4.15188315e+167\n",
      " 1.89133523e+167]\n",
      "Topic: geography\n",
      "[4.98849099e+205 4.98849099e+205 4.98849099e+205 5.15313879e+205\n",
      " 4.98849099e+205]\n",
      "Topic: society\n",
      "[1.10844416e+106 1.04929651e+106 1.23900060e+106 1.04929651e+106\n",
      " 1.28248477e+106]\n",
      "Topic: computers\n",
      "[1.59373311e+146 2.05651394e+147 1.59373311e+146 3.26252840e+147\n",
      " 5.36959005e+146]\n",
      "Topic: home\n",
      "[2.07178815e+170 2.07178815e+170 2.26591998e+170 2.36116866e+170\n",
      " 2.35756025e+170]\n",
      "Topic: recreation\n",
      "[6.95123901e+131 5.24018148e+132 6.95123901e+131 8.44471630e+131\n",
      " 6.95123901e+131]\n",
      "Topic: science\n",
      "[1.80775960e+140 1.93710520e+139 2.32234647e+140 8.10786010e+139\n",
      " 2.79323404e+139]\n",
      "Topic: sports\n",
      "[6.83954412e+120 6.83954412e+120 6.83954412e+120 6.83954412e+120\n",
      " 6.83954412e+120]\n",
      "Topic: world\n",
      "[2.79835171e+143 2.86653037e+143 6.70679992e+143 2.79835171e+143\n",
      " 2.79835171e+143]\n"
     ]
    }
   ],
   "source": [
    "# Compute PageRank scores for each topic\n",
    "\n",
    "Topic_names = [\"arts\",\"games\",\"youth\",\"reference\", \"shopping\", \"business\", \"health\", \"news\",\"geography\",\"society\",\"computers\",\"home\",\"recreation\",\"science\",\"sports\",\"world\"]\n",
    "\n",
    "topic_pagerank = {}\n",
    "for topic in Topic_names:\n",
    "    adjacency_matrix = np.load(f'./Matrices/matrice_{topic}.npy')\n",
    "    pagerank_scores = compute_pagerank(adjacency_matrix)\n",
    "    topic_pagerank[topic] = pagerank_scores\n",
    "\n",
    "\n",
    "# Print PageRank scores for each topic\n",
    "for topic, scores in topic_pagerank.items():\n",
    "    print(f\"Topic: {topic}\")\n",
    "    print(scores[:5])\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Similarity ranking (4.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def OSim(t1, t2, n=20):\n",
    "    # t1, t2 listes\n",
    "    return len(set(t1[:n])&set(t2[:n]))/n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "def KSim(t1, t2):\n",
    "    # t1, t2 listes\n",
    "    U = set(t1)|set(t2)\n",
    "    delta1 = U - set(t1)\n",
    "    delta2 = U - set(t2)\n",
    "    t1_prime = t1+list(delta1)\n",
    "    t2_prime = t1+list(delta2)\n",
    "    sim = 0\n",
    "    for u,v in list(itertools.permutations(U, 2)):\n",
    "        if np.sign(t1_prime.index(u)-t1_prime.index(v))==np.sign(t2_prime.index(u)-t2_prime.index(v)):\n",
    "            sim+=1\n",
    "    return sim/(len(U)*(len(U)-1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query-time importance score (3.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calcul de D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A faire : changer le nom de fichiers json pour qu'ils soient enregistrés sous le nom de leur titre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "\n",
    "categories_text = []\n",
    "for c in topics.keys():\n",
    "    text=\"\"\n",
    "    for article in topics[c]:\n",
    "        with open(f\"{article}.json\") as json_file:\n",
    "            json_data = json.load(json_file)\n",
    "            text+=\" \"+json_data.text\n",
    "        categories_text.append(text)\n",
    "\n",
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "D = vectorizer.fit_transform(categories_text)\n",
    "terms_indexes = vectorizer.get_feature_names_out()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# à changer pour ne calculer que sur les docs qui ont les termes de la query au lieu de tout\n",
    "\n",
    "def sqd(q, topic_pagerank, D, terms_indexes):\n",
    "    # D = gives the total number of occurrences of each term in each document, array of shape = classes x terms\n",
    "    \n",
    "    p_c = 1/16 # \"The quantity P(cj) is not as straightforward. We chose to make it uniform\"\n",
    "    \n",
    "    indexes = np.in1d(terms_indexes, np.array(q)).nonzero()[0]\n",
    "\n",
    "    p_q_c = D[indexes]/D.sum(axis=1)\n",
    "\n",
    "    probas_c_q = p_c * np.product(p_q_c)\n",
    "\n",
    "    return np.sum(probas_c_q*topic_pagerank,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = [ \"affirmative action\", \"alcoholism\", \"amusement parks\", \n",
    "            \"architecture\", \"bicycling\", \"blues\", \"cheese\", \n",
    "            \"citrus groves\", \"classical guitar\", \"computer vision\", \n",
    "            \"cruises\", \"death valley\", \"field hockey\", \n",
    "            \"gardening\", \"graphic design\", \"gulf war\", \n",
    "            \"hiv\", \"java\", \"lipari\",\n",
    "            \"lyme disease\", \"mutual funds\", \"national parks\", \n",
    "            \"parallel architecture\", \"recycling cans\", \"rock climbing\", \n",
    "            \"san francisco\", \"shakespeare\", \"stamp collecting\", \n",
    "            \"sushi\", \"table tennis\", \"telecommuting\", \n",
    "            \"vintage cars\", \"volcano\", \"zen buddhism\", \"zener\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
