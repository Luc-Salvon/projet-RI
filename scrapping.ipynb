{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install wikipedia-api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wikipediaapi\n",
    "\n",
    "\n",
    "wiki_wiki = wikipediaapi.Wikipedia(\n",
    "    user_agent='MyProjectName',\n",
    "        language='en',\n",
    "        extract_format=wikipediaapi.ExtractFormat.WIKI\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Get data for each category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import shuffle\n",
    "\n",
    "\n",
    "def url_categorymembers(categorymembers,list_pages=[], level=0, max_level=3, i=0):\n",
    "    pages_cat = list(categorymembers.values()) #toutes les pages de la catégorie\n",
    "    shuffle(pages_cat)\n",
    "\n",
    "    for c in pages_cat:\n",
    "        if i==1000: #1000 pages par catégorie\n",
    "            break\n",
    "\n",
    "        if c.title[:6]!=\"Portal\" and c.title[:5]!=\"File:\" and c.title[:8]!=\"Category\": # vérifie que c'est bien un article\n",
    "            i+=1\n",
    "            d = {}\n",
    "            d[\"titre\"] = c.title\n",
    "            d[\"contenu\"] = c.text\n",
    "            d[\"liens\"] = list(c.links.keys())\n",
    "            list_pages.append(d)\n",
    "\n",
    "        if c.ns == wikipediaapi.Namespace.CATEGORY and level < max_level: # descend d'un niveau\n",
    "            i = url_categorymembers(c.categorymembers, list_pages, level=level + 1, max_level=max_level, i=i)[1]\n",
    "\n",
    "    return list_pages, i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_arts = wiki_wiki.page(\"Category:The arts\") # Arts\n",
    "arts, _ = url_categorymembers(cat_arts.categorymembers)\n",
    "pd.DataFrame(arts).to_csv('arts.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_games = wiki_wiki.page(\"Category:Games\") # Games\n",
    "games, _= url_categorymembers(cat_games.categorymembers)\n",
    "\n",
    "pd.DataFrame(games).to_csv('games.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_youth = wiki_wiki.page(\"Category:Youth\") # Kids and Teens (not exact)\n",
    "youth, _ = url_categorymembers(cat_youth.categorymembers)\n",
    "\n",
    "pd.DataFrame(youth).to_csv('youth.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_reference = wiki_wiki.page(\"Category:Reference\") # Reference\n",
    "reference, _ = url_categorymembers(cat_reference.categorymembers)\n",
    "\n",
    "pd.DataFrame(reference).to_csv('reference.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_shopping = wiki_wiki.page(\"Category:Retailing\") # Shopping\n",
    "shopping, _ = url_categorymembers(cat_shopping.categorymembers)\n",
    "\n",
    "pd.DataFrame(shopping).to_csv('shopping.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_business = wiki_wiki.page(\"Category:Business\") # Business\n",
    "business, _ = url_categorymembers(cat_business.categorymembers)\n",
    "\n",
    "pd.DataFrame(business).to_csv('business.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_health = wiki_wiki.page(\"Category:Health\") # Health\n",
    "health, _ = url_categorymembers(cat_health.categorymembers)\n",
    "\n",
    "pd.DataFrame(health).to_csv('health.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_news = wiki_wiki.page(\"Category:News\") # News\n",
    "news, _ = url_categorymembers(cat_news.categorymembers)\n",
    "\n",
    "pd.DataFrame(news).to_csv('news.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_geography = wiki_wiki.page(\"Category:Geography\") # Regional (not exact)\n",
    "geography, _ = url_categorymembers(cat_geography.categorymembers)\n",
    "\n",
    "pd.DataFrame(geography).to_csv('geography.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_society = wiki_wiki.page(\"Category:Society\") # Society\n",
    "society, _ = url_categorymembers(cat_society.categorymembers)\n",
    "\n",
    "pd.DataFrame(society).to_csv('society.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_computers = wiki_wiki.page(\"Category:Computers\") # Computers\n",
    "computers, _ = url_categorymembers(cat_computers.categorymembers)\n",
    "\n",
    "pd.DataFrame(computers).to_csv('computers.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_home = wiki_wiki.page(\"Category:Home\") # Home\n",
    "home, _ = url_categorymembers(cat_home.categorymembers)\n",
    "\n",
    "pd.DataFrame(home).to_csv('home.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_recreation = wiki_wiki.page(\"Category:Recreation\") # Recreation\n",
    "recreation, _ = url_categorymembers(cat_recreation.categorymembers)\n",
    "\n",
    "pd.DataFrame(recreation).to_csv('recreation.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_science = wiki_wiki.page(\"Category:Science\") # Science\n",
    "science, _ = url_categorymembers(cat_science.categorymembers)\n",
    "\n",
    "pd.DataFrame(science).to_csv('science.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_sports = wiki_wiki.page(\"Category:Sports\") # Sports\n",
    "sports, _ = url_categorymembers(cat_sports.categorymembers)\n",
    "\n",
    "pd.DataFrame(sports).to_csv('sports.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_world = wiki_wiki.page(\"Category:World\") # World\n",
    "world, _ = url_categorymembers(cat_world.categorymembers)\n",
    "\n",
    "pd.DataFrame(world).to_csv('world.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_files = [\n",
    "    'arts.csv', 'business.csv', 'computers.csv', 'games.csv',\n",
    "    'geography.csv', 'health.csv', 'home.csv', 'news.csv',\n",
    "    'recreation.csv', 'reference.csv', 'science.csv', 'shopping.csv',\n",
    "    'society.csv', 'sports.csv', 'world.csv', 'youth.csv'\n",
    "]\n",
    "\n",
    "# Liste pour stocker les DataFrames\n",
    "dataframes = []\n",
    "\n",
    "# Lire chaque fichier CSV et l'ajouter à la liste des DataFrames\n",
    "for file in csv_files:\n",
    "    df = pd.read_csv(f'./Data/{file}')\n",
    "    dataframes.append(df)\n",
    "\n",
    "# Combiner tous les DataFrames en un seul\n",
    "combined_df = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "# Écrire le DataFrame combiné dans un nouveau fichier CSV\n",
    "combined_df.to_csv('./Data/all.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute Page Rank vectors (3.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrices d'adjacence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construction de la matrice sans utiliser les fichiers :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the adjacency matrix\n",
    "def build_adjacency_matrix_2(df): #pas optimisé\n",
    "    num_pages = df.shape[0]\n",
    "    adjacency_matrix = np.zeros((num_pages, num_pages))\n",
    "    df[\"liens\"] = df[\"liens\"].apply(ast.literal_eval)\n",
    "    \n",
    "    for index, row in df.iterrows():\n",
    "        N = len(np.where(np.isin(row['liens'],df['titre'].values))[0]) # nombres de liens que fait la page\n",
    "        for link in row['liens']:\n",
    "            if link in df['titre'].values:\n",
    "                j = np.where(df['titre'] == link )[0]\n",
    "                adjacency_matrix[j,index] += 1/N\n",
    "\n",
    "    return adjacency_matrix\n",
    "\n",
    "\n",
    "\n",
    "def build_adjacency_matrix(df):\n",
    "    df[\"liens\"] = df[\"liens\"].apply(ast.literal_eval)  # Convertir les liens de type string en listes\n",
    "\n",
    "    # Créer un dictionnaire pour mapper les titres aux indices\n",
    "    title_to_index = {title: i for i, title in enumerate(df['titre'])}\n",
    "    \n",
    "    # Initialiser la matrice d'adjacence avec des zéros\n",
    "    num_pages = len(df)\n",
    "    adjacency_matrix = np.zeros((num_pages, num_pages))\n",
    "    \n",
    "    # Construire une série pour mapper les liens aux indices\n",
    "    link_series = pd.Series(df['liens'].values.tolist())\n",
    "    link_indices = link_series.map(lambda x: [title_to_index[link] for link in x if link in title_to_index])\n",
    "    \n",
    "    # Compter le nombre de liens sortants pour chaque article\n",
    "    df['num_liens'] = link_indices.apply(len)\n",
    "    \n",
    "    for i, indices in enumerate(link_indices):\n",
    "        if len(indices)>0:\n",
    "            adjacency_matrix[indices, i] = 1 / df.at[i, 'num_liens']\n",
    "    \n",
    "    return adjacency_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrice_all= build_adjacency_matrix(pd.read_csv('./Data/all.csv'))\n",
    "np.save('./Matrices/matrice_all.npy', matrice_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construction de la matrice en utilisant les fichiers :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fichier_graphe = pd.read_csv('enwiki.wikilink_graph.2018-03-01.csv')\n",
    "\n",
    "# build the adjacency matrix\n",
    "def build_adjacency_matrix(topic_articles, inversed_index_articles):\n",
    "    num_pages = len(topic_articles)\n",
    "    adjacency_matrix = np.zeros((num_pages, num_pages))\n",
    "\n",
    "    for i,titre in enumerate(topic_articles):\n",
    "        #links = page_links(pages[i])\n",
    "        links = fichier_graphe[fichier_graphe['page_title_from']==titre]\n",
    "        for link in links:\n",
    "            if link in topic_articles:\n",
    "                j = topic_articles.index(link)\n",
    "                adjacency_matrix[j][i] = 1\n",
    "\n",
    "    return adjacency_matrix/sum(adjacency_matrix,axis=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calcul des vecteurs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute PageRank scores\n",
    "def compute_pagerank(M, alpha=0.25, bias = None): # bias = None pour page_rank normal, bias = topic sinon\n",
    "    # M = adjacency_matrix\n",
    "    # damping_factor = 1-alpha\n",
    "\n",
    "    N = M.shape[0]\n",
    "\n",
    "    if bias is None : # page_rank normal, pas topic_sensitive\n",
    "        p = np.ones(N) / N \n",
    "\n",
    "    else: # ODP-biasing \n",
    "        articles = pd.read_csv(f'./Data/all.csv')[\"titre\"].values\n",
    "        articles_cat = pd.read_csv(f'./Data/{bias}.csv')[\"titre\"].values\n",
    "        p = np.where(np.isin(articles,articles_cat), 1/len(articles_cat), 0)\n",
    "\n",
    "    #M_prime = (1-alpha) * M + alpha * teleportation_matrix\n",
    "\n",
    "    rank = np.ones(N) / N\n",
    "    old_rank = np.zeros(N)\n",
    "\n",
    "    epsilon = 1.0e-3\n",
    "    max_iterations = 10\n",
    "    iterations = 0\n",
    "\n",
    "    while np.sum(np.abs(rank - old_rank)) > epsilon and iterations < max_iterations:\n",
    "        old_rank = rank.copy()\n",
    "        rank = (1-alpha)*M @ rank + p # equation 5\n",
    "        #normaliser ?\n",
    "        iterations += 1\n",
    "\n",
    "    return rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[4], line 5\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# Compute PageRank scores for each topic\u001B[39;00m\n\u001B[1;32m      3\u001B[0m Topic_names \u001B[38;5;241m=\u001B[39m [\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124marts\u001B[39m\u001B[38;5;124m\"\u001B[39m,\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mgames\u001B[39m\u001B[38;5;124m\"\u001B[39m,\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124myouth\u001B[39m\u001B[38;5;124m\"\u001B[39m,\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mreference\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mshopping\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbusiness\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhealth\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnews\u001B[39m\u001B[38;5;124m\"\u001B[39m,\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mgeography\u001B[39m\u001B[38;5;124m\"\u001B[39m,\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msociety\u001B[39m\u001B[38;5;124m\"\u001B[39m,\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcomputers\u001B[39m\u001B[38;5;124m\"\u001B[39m,\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhome\u001B[39m\u001B[38;5;124m\"\u001B[39m,\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrecreation\u001B[39m\u001B[38;5;124m\"\u001B[39m,\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mscience\u001B[39m\u001B[38;5;124m\"\u001B[39m,\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msports\u001B[39m\u001B[38;5;124m\"\u001B[39m,\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mworld\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[0;32m----> 5\u001B[0m adjacency_matrix \u001B[38;5;241m=\u001B[39m \u001B[43mnp\u001B[49m\u001B[38;5;241m.\u001B[39mload(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m./Matrices/matrice_all.npy\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m      7\u001B[0m topic_pagerank \u001B[38;5;241m=\u001B[39m {}\n\u001B[1;32m      8\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m topic \u001B[38;5;129;01min\u001B[39;00m Topic_names:\n\u001B[1;32m      9\u001B[0m     \u001B[38;5;66;03m#adjacency_matrix = np.load(f'./Matrices/matrice_{topic}.npy')\u001B[39;00m\n",
      "\u001B[0;31mNameError\u001B[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "# Compute PageRank scores for each topic\n",
    "import numpy as np\n",
    "\n",
    "Topic_names = [\"arts\",\"games\",\"youth\",\"reference\", \"shopping\", \"business\", \"health\", \"news\",\"geography\",\"society\",\"computers\",\"home\",\"recreation\",\"science\",\"sports\",\"world\"]\n",
    "\n",
    "adjacency_matrix = np.load(f'./Matrices/matrice_all.npy')\n",
    "\n",
    "topic_pagerank = {}\n",
    "for topic in Topic_names:\n",
    "    #adjacency_matrix = np.load(f'./Matrices/matrice_{topic}.npy')\n",
    "    \n",
    "    pagerank_scores = compute_pagerank(adjacency_matrix,bias=topic)\n",
    "\n",
    "    print(f\"Topic: {topic}\")\n",
    "    print(pagerank_scores[:5])\n",
    "\n",
    "    topic_pagerank[topic] = pagerank_scores\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('topic_pagerank.pkl', 'wb') as fichier:\n",
    "    pickle.dump(topic_pagerank, fichier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adjacency_matrix = np.load(f'./Matrices/matrice_all.npy')\n",
    "pagerank_normal = compute_pagerank(adjacency_matrix,bias=None)\n",
    "with open('normal_pagerank.pkl', 'wb') as fichier:\n",
    "    pickle.dump(pagerank_normal, fichier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query-time importance score (3.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calcul de D"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T22:54:48.508273Z",
     "start_time": "2024-05-21T22:54:44.847150Z"
    }
   },
   "source": [
    "import json\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "\n",
    "categories_text = {}\n",
    "for c in Topic_names:\n",
    "    liste_textes = pd.read_csv(f'./Data/{c}.csv')[\"contenu\"].values.astype(str)\n",
    "\n",
    "    texte = ' '.join(liste_textes)\n",
    "    categories_text[c] = texte\n",
    "\n",
    "D_categories = {}\n",
    "terms_indexes_categories = {}\n",
    "\n",
    "for c, text in categories_text.items():\n",
    "    vectorizer = CountVectorizer(stop_words='english')\n",
    "    D_categories[c] = vectorizer.fit_transform([text])\n",
    "    terms_indexes_categories[c] = vectorizer.get_feature_names_out()\n",
    "    "
   ],
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Topic_names' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[1], line 6\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msklearn\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mfeature_extraction\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mtext\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m CountVectorizer\n\u001B[1;32m      5\u001B[0m categories_text \u001B[38;5;241m=\u001B[39m {}\n\u001B[0;32m----> 6\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m c \u001B[38;5;129;01min\u001B[39;00m \u001B[43mTopic_names\u001B[49m:\n\u001B[1;32m      7\u001B[0m     liste_textes \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mread_csv(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m./Data/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mc\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.csv\u001B[39m\u001B[38;5;124m'\u001B[39m)[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcontenu\u001B[39m\u001B[38;5;124m\"\u001B[39m]\u001B[38;5;241m.\u001B[39mvalues\u001B[38;5;241m.\u001B[39mastype(\u001B[38;5;28mstr\u001B[39m)\n\u001B[1;32m      9\u001B[0m     texte \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mjoin(liste_textes)\n",
      "\u001B[0;31mNameError\u001B[0m: name 'Topic_names' is not defined"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T22:54:48.510738Z",
     "start_time": "2024-05-21T22:54:48.510677Z"
    }
   },
   "source": [
    "with open('D.pkl', 'wb') as fichier:\n",
    "    pickle.dump(D_categories, fichier)\n",
    "\n",
    "with open('terms_indexes.pkl', 'wb') as fichier:\n",
    "    pickle.dump(terms_indexes_categories, fichier)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install python-terrier"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T22:56:52.382038Z",
     "start_time": "2024-05-21T22:56:47.245360Z"
    }
   },
   "source": [
    "# Indexation des articles \n",
    "import pandas as pd\n",
    "import pyterrier as pt\n",
    "\n",
    "if not pt.started():\n",
    "  pt.init(boot_packages=[\"com.github.terrierteam:terrier-prf:-SNAPSHOT\"])\n",
    "\n",
    "df = pd.read_csv(f'./Data/all.csv')[[\"titre\",\"contenu\"]]\n",
    "df = df.dropna()\n",
    "df.columns = [\"docno\", \"text\"]\n",
    "\n",
    "# pd_indexer = pt.DFIndexer(\"./pd_index\")\n",
    "# indexref = pd_indexer.index(df[\"text\"], df[\"docno\"])\n",
    "# index = pt.IndexFactory.of(indexref)\n",
    "\n",
    "Topic_names = [\"arts\",\"games\",\"youth\",\"reference\", \"shopping\", \"business\", \"health\", \"news\",\"geography\",\"society\",\"computers\",\"home\",\"recreation\",\"science\",\"sports\",\"world\"]"
   ],
   "outputs": [],
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T22:56:57.306668Z",
     "start_time": "2024-05-21T22:56:52.419625Z"
    }
   },
   "source": [
    "df = pd.read_csv(f'./Data/all.csv')[[\"titre\",\"contenu\"]]\n",
    "df = df.fillna('missing')"
   ],
   "outputs": [],
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T23:06:37.176175Z",
     "start_time": "2024-05-21T23:06:37.161155Z"
    }
   },
   "source": [
    "def sqd(q, topic_pagerank, D, terms_indexes, index):\n",
    "    # Using a text index, recuperer les docs qui contiennent q\n",
    "    br = pt.BatchRetrieve(index, wmodel=\"CoordinateMatch\") # CoordinateMatch renvoie 1 si le terme est dans le doc, 0 sinon\n",
    "    docs_retrieved = br.search(q) # Renvoie le nombre de termes de la query qui sont dans chaque doc\n",
    "    docs_query = docs_retrieved[docs_retrieved[\"score\"] == len(q.split())]  # Garder uniquement les docs qui contiennent tous les termes de la query\n",
    "    docs_index = docs_query[\"docno\"].values\n",
    "    \n",
    "    # calculer P(cj|q)\n",
    "    probas_c_q = np.zeros((16))\n",
    "    p_c = 1/16 # \"The quantity P(cj) is not as straightforward. We chose to make it uniform\"\n",
    "\n",
    "    for i,topic in enumerate(Topic_names):\n",
    "        indexes = np.in1d(terms_indexes[topic], np.array(q)).nonzero()[0]\n",
    "\n",
    "        if len(indexes)==0:\n",
    "            probas_c_q[i] = 0\n",
    "        else:\n",
    "            p_q_c = D[topic].tocsc()[indexes]/D[topic].tocsc().sum()\n",
    "            p_q_c = p_q_c.toarray()\n",
    "            probas_c_q[i] = p_c * np.prod(p_q_c)\n",
    "            \n",
    "    df = pd.read_csv(f'./Data/all.csv')\n",
    "    df.columns = [\"id\", \"titre\", \"contenu\", \"liens\"]\n",
    "            \n",
    "    docs = df[df[\"titre\"].isin(docs_index)].copy()\n",
    "    \n",
    "    # print(docs)\n",
    "    \n",
    "    docs[\"score\"] = docs.apply(lambda x: sum([probas_c_q[Topic_names.index(topic)]*topic_pagerank[topic][x[\"id\"]] for topic in Topic_names]), axis=1)\n",
    "    \n",
    "    # print(docs)\n",
    "\n",
    "    # garder les 3 meilleurs cj\n",
    "    # sorted_indices = np.argsort(-probas_c_q)\n",
    "    # \n",
    "    # top_indices = sorted_indices[:3]\n",
    "    # top_values = probas_c_q[top_indices]\n",
    "    # \n",
    "    # # somme des P(cj|q) * rank j\n",
    "    # res = 0\n",
    "    # for i in range(3):\n",
    "    #     res+= top_values[i]*topic_pagerank[Topic_names[top_indices[i]]]\n",
    "    #return np.sum(top_values*topic_pagerank[top_indices],axis=1)\n",
    "\n",
    "\n",
    "    # meilleurs documents :\n",
    "\n",
    "    # doc qui ont les mots de la query sans pyterrier\n",
    "    # df['text_lower'] = df['contenu'].str.lower() # Convert the text column to lowercase for case-insensitive search\n",
    "    # query_words = q.lower().split() # Split the query into individual words\n",
    "    # mask = df['text_lower'].str.contains('|'.join(query_words)) # boolean mask to filter rows containing any of the query words\n",
    "    # doc_indices = df.index[mask] # indices des docs qui ont la query\n",
    "    \n",
    "    #res[doc_indices]+=100 # on leur donne plus de valeur pour qu'ils sortent d'abord\n",
    "\n",
    "    #sorted_docs_index = np.argsort(-res)\n",
    "    # best_docs = pd.read_csv(f'./Data/all.csv')[\"titre\"].values[sorted_docs_index]\n",
    "\n",
    "    best_docs = docs.sort_values(by=\"score\", ascending=False)[\"titre\"]\n",
    "    return best_docs"
   ],
   "outputs": [],
   "execution_count": 37
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T22:56:57.332067Z",
     "start_time": "2024-05-21T22:56:57.328703Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# query = \"affirmative action\"\n",
    "# print(sqd(query, topic_pagerank, D, terms_indexes, index)[:5])"
   ],
   "outputs": [],
   "execution_count": 20
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T23:08:37.139589Z",
     "start_time": "2024-05-21T23:08:37.129935Z"
    }
   },
   "source": [
    "def score_normal_pagerank(q,normal_pagerank):\n",
    "    \n",
    "    # # doc qui ont les mots de la query (sans pyterrier)\n",
    "    # df['text_lower'] = df['contenu'].str.lower() # Convert the text column to lowercase for case-insensitive search\n",
    "    # query_words = q.lower().split() # Split the query into individual words\n",
    "    # mask = df['text_lower'].str.contains('|'.join(query_words)) # boolean mask to filter rows containing any of the query words\n",
    "    # doc_indices = df.index[mask] # indices des docs qui ont la query\n",
    "    # normal_pagerank[doc_indices] +=100\n",
    "    # \n",
    "    # sorted_docs_index = np.argsort(-normal_pagerank)\n",
    "    # best_docs = pd.read_csv(f'./Data/all.csv')[\"titre\"].values[sorted_docs_index]\n",
    "    \n",
    "    # Using a text index, recuperer les docs qui contiennent q\n",
    "    br = pt.BatchRetrieve(index, wmodel=\"CoordinateMatch\") # CoordinateMatch renvoie 1 si le terme est dans le doc, 0 sinon\n",
    "    docs_retrieved = br.search(q) # Renvoie le nombre de termes de la query qui sont dans chaque doc\n",
    "    docs_query = docs_retrieved[docs_retrieved[\"score\"] == len(q.split())]  # Garder uniquement les docs qui contiennent tous les termes de la query\n",
    "    sorted_index = docs_query[\"docno\"].values\n",
    "    \n",
    "    df = pd.read_csv(f'./Data/all.csv')\n",
    "    df.columns = [\"id\", \"titre\", \"contenu\", \"liens\"]\n",
    "    \n",
    "    docs = df[df[\"titre\"].isin(sorted_index)].copy()\n",
    "    docs[\"score\"] = normal_pagerank[docs[\"id\"].values]\n",
    "    \n",
    "    best_docs = docs.sort_values(by=\"score\", ascending=False)[\"titre\"]\n",
    "    \n",
    "    return best_docs\n"
   ],
   "outputs": [],
   "execution_count": 41
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimental results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarity ranking (4.1)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T22:56:57.366004Z",
     "start_time": "2024-05-21T22:56:57.359872Z"
    }
   },
   "source": [
    "def OSim(t1, t2, n=20):\n",
    "    # t1, t2 listes\n",
    "    return len(set(t1[:n])&set(t2[:n]))/n"
   ],
   "outputs": [],
   "execution_count": 22
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T22:56:57.380748Z",
     "start_time": "2024-05-21T22:56:57.372535Z"
    }
   },
   "source": [
    "import itertools\n",
    "\n",
    "def KSim(t1, t2):\n",
    "    # t1, t2 listes\n",
    "    U = set(t1)|set(t2)\n",
    "    delta1 = U - set(t1)\n",
    "    delta2 = U - set(t2)\n",
    "\n",
    "    t1_prime = list(t1)+list(delta1)\n",
    "    t2_prime = list(t2)+list(delta2)\n",
    "\n",
    "    sim = 0\n",
    "    for u,v in list(itertools.permutations(U, 2)):\n",
    "        if np.sign(t1_prime.index(u)-t1_prime.index(v))==np.sign(t2_prime.index(u)-t2_prime.index(v)):\n",
    "            sim+=1\n",
    "    return sim/(len(U)*(len(U)-1))\n"
   ],
   "outputs": [],
   "execution_count": 23
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tests on queries"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T22:56:57.422167Z",
     "start_time": "2024-05-21T22:56:57.408916Z"
    }
   },
   "source": [
    "queries = [ \"affirmative action\", \"alcoholism\", \"amusement parks\", \n",
    "            \"architecture\", \"bicycling\", \"blues\", \"cheese\", \n",
    "            \"citrus groves\", \"classical guitar\", \"computer vision\", \n",
    "            \"cruises\", \"death valley\", \"field hockey\", \n",
    "            \"gardening\", \"graphic design\", \"gulf war\", \n",
    "            \"hiv\", \"java\", \"lipari\",\n",
    "            \"lyme disease\", \"mutual funds\", \"national parks\", \n",
    "            \"parallel architecture\", \"recycling cans\", \"rock climbing\", \n",
    "            \"san francisco\", \"shakespeare\", \"stamp collecting\", \n",
    "            \"sushi\", \"table tennis\", \"telecommuting\", \n",
    "            \"vintage cars\", \"volcano\", \"zen buddhism\", \"zener\"]"
   ],
   "outputs": [],
   "execution_count": 24
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T23:04:49.471778Z",
     "start_time": "2024-05-21T23:04:49.002235Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "with open(\"terms_indexes.pkl\", \"rb\") as f:\n",
    "    terms_indexes = pickle.load(f)\n",
    "with open(\"D.pkl\", \"rb\") as f:\n",
    "    D = pickle.load(f)\n",
    "    for topic in Topic_names:\n",
    "        D[topic] = np.reshape(D[topic],(D[topic].shape[1],1))\n",
    "with open(\"topic_pagerank.pkl\", \"rb\") as f:\n",
    "    topic_pagerank = pickle.load(f)\n",
    "    \n",
    "index = pt.IndexFactory.of(\"./pd_index\")"
   ],
   "outputs": [],
   "execution_count": 33
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T23:06:57.693851Z",
     "start_time": "2024-05-21T23:06:47.341616Z"
    }
   },
   "source": [
    "for q in queries[:5]:\n",
    "    print(q)\n",
    "    print(sqd(q, topic_pagerank, D, terms_indexes, index)[:5])\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "affirmative action\n",
      "8        Expressive therapies\n",
      "11499        Apple and unions\n",
      "12423           Shadow family\n",
      "12384           Public sphere\n",
      "12357    Gender mainstreaming\n",
      "Name: titre, dtype: object\n",
      "alcoholism\n",
      "15584           Shameless (British TV series)\n",
      "13729                     I will moida da bum\n",
      "8276                 Violence and video games\n",
      "5827                      Million Women Study\n",
      "10992    List of Stargate Universe characters\n",
      "Name: titre, dtype: object\n",
      "amusement parks\n",
      "85         Dream world (plot device)\n",
      "8879                            Park\n",
      "8762                         Soarin'\n",
      "8758    Lists of tourist attractions\n",
      "8743              Tourist attraction\n",
      "Name: titre, dtype: object\n",
      "architecture\n",
      "750                          Art\n",
      "8750                      Museum\n",
      "10584      Mapping controversies\n",
      "12584    Heritage interpretation\n",
      "5584         Imaging informatics\n",
      "Name: titre, dtype: object\n",
      "bicycling\n",
      "6273                                        Pannier\n",
      "758                                  Conceptual art\n",
      "2827                                          Intel\n",
      "14841               Global Alliance for EcoMobility\n",
      "4843     List of Colorado geographic features lists\n",
      "Name: titre, dtype: object\n"
     ]
    }
   ],
   "execution_count": 38
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T23:07:58.842977Z",
     "start_time": "2024-05-21T23:07:58.832081Z"
    }
   },
   "source": [
    "with open(\"normal_pagerank.pkl\", \"rb\") as f:\n",
    "        normal_pagerank = pickle.load(f)"
   ],
   "outputs": [],
   "execution_count": 39
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T23:08:55.917144Z",
     "start_time": "2024-05-21T23:08:42.377363Z"
    }
   },
   "source": [
    "for q in queries[:5]:\n",
    "    print(q)\n",
    "    print(score_normal_pagerank(q, normal_pagerank)[:5])"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "affirmative action\n",
      "14017    Universal Declaration of Human Rights\n",
      "10992     List of Stargate Universe characters\n",
      "7810                           Murray Rothbard\n",
      "14841          Global Alliance for EcoMobility\n",
      "7101                               Jactitation\n",
      "Name: titre, dtype: object\n",
      "alcoholism\n",
      "15584    Shameless (British TV series)\n",
      "13729              I will moida da bum\n",
      "5827               Million Women Study\n",
      "8276          Violence and video games\n",
      "6156                      Chafing dish\n",
      "Name: titre, dtype: object\n",
      "amusement parks\n",
      "8584                            Hobby\n",
      "8758     Lists of tourist attractions\n",
      "14148                       Ice cream\n",
      "7748              Japanese war crimes\n",
      "10994                 Kenny McCormick\n",
      "Name: titre, dtype: object\n",
      "architecture\n",
      "750                          Art\n",
      "8750                      Museum\n",
      "12584    Heritage interpretation\n",
      "10584      Mapping controversies\n",
      "5584         Imaging informatics\n",
      "Name: titre, dtype: object\n",
      "bicycling\n",
      "6273                                        Pannier\n",
      "758                                  Conceptual art\n",
      "2827                                          Intel\n",
      "14841               Global Alliance for EcoMobility\n",
      "4843     List of Colorado geographic features lists\n",
      "Name: titre, dtype: object\n"
     ]
    }
   ],
   "execution_count": 42
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T23:09:08.733438Z",
     "start_time": "2024-05-21T23:08:55.918721Z"
    }
   },
   "source": [
    "for q in queries[:5]:\n",
    "    print(q)\n",
    "    print(score_normal_pagerank(q, topic_pagerank['business'])[:5])"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "affirmative action\n",
      "10992     List of Stargate Universe characters\n",
      "14017    Universal Declaration of Human Rights\n",
      "9991          Chronicle of the King D. Pedro I\n",
      "7101                               Jactitation\n",
      "748                              Theory of art\n",
      "Name: titre, dtype: object\n",
      "alcoholism\n",
      "6922                                   Skewer\n",
      "12922                            Public enemy\n",
      "11958                                  Sandoz\n",
      "8969                               Maxim Wien\n",
      "10992    List of Stargate Universe characters\n",
      "Name: titre, dtype: object\n",
      "amusement parks\n",
      "10958                          Tom Swift\n",
      "8136               Video games and Linux\n",
      "3996     Characters of the Tekken series\n",
      "8758        Lists of tourist attractions\n",
      "10994                    Kenny McCormick\n",
      "Name: titre, dtype: object\n",
      "architecture\n",
      "8962                                 Reserve design\n",
      "969                                          Design\n",
      "272                                       Patronage\n",
      "3272    List of Xbox games compatible with Xbox 360\n",
      "8750                                         Museum\n",
      "Name: titre, dtype: object\n",
      "bicycling\n",
      "2827                      Intel\n",
      "13955          Element (sports)\n",
      "8109         Humans vs. Zombies\n",
      "6273                    Pannier\n",
      "9302     Guinness World Records\n",
      "Name: titre, dtype: object\n"
     ]
    }
   ],
   "execution_count": 43
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T23:09:18.447306Z",
     "start_time": "2024-05-21T23:09:08.734693Z"
    }
   },
   "source": [
    "for q in queries[:5]:\n",
    "    print(q)\n",
    "    print(score_normal_pagerank(q, topic_pagerank['sports'])[:5])"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "affirmative action\n",
      "14017    Universal Declaration of Human Rights\n",
      "7810                           Murray Rothbard\n",
      "7101                               Jactitation\n",
      "10992     List of Stargate Universe characters\n",
      "14841          Global Alliance for EcoMobility\n",
      "Name: titre, dtype: object\n",
      "alcoholism\n",
      "5827              Million Women Study\n",
      "5934     Heart failure classification\n",
      "8969                       Maxim Wien\n",
      "11957                           Maggi\n",
      "5926                   Medical record\n",
      "Name: titre, dtype: object\n",
      "amusement parks\n",
      "8758     Lists of tourist attractions\n",
      "8584                            Hobby\n",
      "10958                       Tom Swift\n",
      "14148                       Ice cream\n",
      "10994                 Kenny McCormick\n",
      "Name: titre, dtype: object\n",
      "architecture\n",
      "750                                              Art\n",
      "8750                                          Museum\n",
      "2827                                           Intel\n",
      "8962                                  Reserve design\n",
      "10934    List of things named after John von Neumann\n",
      "Name: titre, dtype: object\n",
      "bicycling\n",
      "2827                                Intel\n",
      "1934                    Domar aggregation\n",
      "13955                    Element (sports)\n",
      "6273                              Pannier\n",
      "1700     American system of manufacturing\n",
      "Name: titre, dtype: object\n"
     ]
    }
   ],
   "execution_count": 44
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarité entre les ranking de pagerank normal et de pagerank avec topic"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T23:11:40.913742Z",
     "start_time": "2024-05-21T23:09:32.121662Z"
    }
   },
   "source": [
    "for q in queries:\n",
    "    normal = score_normal_pagerank(q, normal_pagerank)\n",
    "    topic = sqd(q, topic_pagerank, D, terms_indexes, index)\n",
    "    print(q,\"\\tOSim:\",OSim(normal, topic, n=20))\n",
    "    #print(\"KSim,\",KSim(normal[:20], topic[:20]))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "affirmative action \tOSim: 0.1\n",
      "alcoholism \tOSim: 0.9\n",
      "amusement parks \tOSim: 0.2\n",
      "architecture \tOSim: 1.0\n",
      "bicycling \tOSim: 0.9\n",
      "blues \tOSim: 0.85\n",
      "cheese \tOSim: 0.7\n",
      "citrus groves \tOSim: 0.15\n",
      "classical guitar \tOSim: 0.55\n",
      "computer vision \tOSim: 0.1\n",
      "cruises \tOSim: 0.75\n",
      "death valley \tOSim: 0.2\n",
      "field hockey \tOSim: 0.15\n",
      "gardening \tOSim: 0.6\n",
      "graphic design \tOSim: 0.0\n",
      "gulf war \tOSim: 0.35\n",
      "hiv \tOSim: 0.8\n",
      "java \tOSim: 0.85\n",
      "lipari \tOSim: 0.05\n",
      "lyme disease \tOSim: 0.2\n",
      "mutual funds \tOSim: 0.15\n",
      "national parks \tOSim: 0.0\n",
      "parallel architecture \tOSim: 0.3\n",
      "recycling cans \tOSim: 0.0\n",
      "rock climbing \tOSim: 0.55\n",
      "san francisco \tOSim: 0.0\n",
      "shakespeare \tOSim: 0.95\n",
      "stamp collecting \tOSim: 0.3\n",
      "sushi \tOSim: 0.85\n",
      "table tennis \tOSim: 0.35\n",
      "telecommuting \tOSim: 0.1\n",
      "vintage cars \tOSim: 0.9\n",
      "volcano \tOSim: 0.85\n",
      "zen buddhism \tOSim: 0.5\n",
      "zener \tOSim: 0.15\n"
     ]
    }
   ],
   "execution_count": 46
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T23:12:45.266382Z",
     "start_time": "2024-05-21T23:11:40.917263Z"
    }
   },
   "source": [
    "for q in queries:\n",
    "    normal = score_normal_pagerank(q, normal_pagerank)\n",
    "    topic = sqd(q, topic_pagerank, D, terms_indexes, index)\n",
    "    print(q,\"\\tKSim,\",KSim(normal[:20], topic[:20]))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "affirmative action \tKSim, 0.2927927927927928\n",
      "alcoholism \tKSim, 0.8484848484848485\n",
      "amusement parks \tKSim, 0.3548387096774194\n",
      "architecture \tKSim, 0.9526315789473684\n",
      "bicycling \tKSim, 0.8831168831168831\n",
      "blues \tKSim, 0.8023715415019763\n",
      "cheese \tKSim, 0.7476923076923077\n",
      "citrus groves \tKSim, 0.3333333333333333\n",
      "classical guitar \tKSim, 0.45454545454545453\n",
      "computer vision \tKSim, 0.24444444444444444\n",
      "cruises \tKSim, 0.6633333333333333\n",
      "death valley \tKSim, 0.2838709677419355\n",
      "field hockey \tKSim, 0.3092436974789916\n",
      "gardening \tKSim, 0.5079365079365079\n",
      "graphic design \tKSim, 0.21923076923076923\n",
      "gulf war \tKSim, 0.3314393939393939\n",
      "hiv \tKSim, 0.8268398268398268\n",
      "java \tKSim, 0.7707509881422925\n"
     ]
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mZeroDivisionError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[47], line 4\u001B[0m\n\u001B[1;32m      2\u001B[0m normal \u001B[38;5;241m=\u001B[39m score_normal_pagerank(q, normal_pagerank)\n\u001B[1;32m      3\u001B[0m topic \u001B[38;5;241m=\u001B[39m sqd(q, topic_pagerank, D, terms_indexes, index)\n\u001B[0;32m----> 4\u001B[0m \u001B[38;5;28mprint\u001B[39m(q,\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\t\u001B[39;00m\u001B[38;5;124mKSim,\u001B[39m\u001B[38;5;124m\"\u001B[39m,\u001B[43mKSim\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnormal\u001B[49m\u001B[43m[\u001B[49m\u001B[43m:\u001B[49m\u001B[38;5;241;43m20\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtopic\u001B[49m\u001B[43m[\u001B[49m\u001B[43m:\u001B[49m\u001B[38;5;241;43m20\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m)\n",
      "Cell \u001B[0;32mIn[23], line 16\u001B[0m, in \u001B[0;36mKSim\u001B[0;34m(t1, t2)\u001B[0m\n\u001B[1;32m     14\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m np\u001B[38;5;241m.\u001B[39msign(t1_prime\u001B[38;5;241m.\u001B[39mindex(u)\u001B[38;5;241m-\u001B[39mt1_prime\u001B[38;5;241m.\u001B[39mindex(v))\u001B[38;5;241m==\u001B[39mnp\u001B[38;5;241m.\u001B[39msign(t2_prime\u001B[38;5;241m.\u001B[39mindex(u)\u001B[38;5;241m-\u001B[39mt2_prime\u001B[38;5;241m.\u001B[39mindex(v)):\n\u001B[1;32m     15\u001B[0m         sim\u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m\n\u001B[0;32m---> 16\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43msim\u001B[49m\u001B[38;5;241;43m/\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mlen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mU\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mlen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mU\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m-\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mZeroDivisionError\u001B[0m: division by zero"
     ]
    }
   ],
   "execution_count": 47
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparaison des vecteurs PageRank (table 4)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T23:13:08.693809Z",
     "start_time": "2024-05-21T23:13:08.688230Z"
    }
   },
   "source": [
    "pageranks = topic_pagerank.copy()\n",
    "pageranks[\"normal\"] = normal_pagerank"
   ],
   "outputs": [],
   "execution_count": 48
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T23:14:18.074998Z",
     "start_time": "2024-05-21T23:13:09.462184Z"
    }
   },
   "source": [
    "table2={}\n",
    "for topic_1,topic_2 in list(itertools.permutations([\"normal\"]+Topic_names, 2)):\n",
    "    print(topic_1,topic_2)\n",
    "    sim = 0\n",
    "    for q in queries:\n",
    "        indexation1 = score_normal_pagerank(q, pageranks[topic_1])\n",
    "        indexation2 = score_normal_pagerank(q, pageranks[topic_2])\n",
    "        sim += KSim(indexation1[:20], indexation2[:20])\n",
    "    sim /= len(queries)\n",
    "    table2 = {f\"{topic_1}/{topic_2}\": sim}\n",
    "\n",
    "    "
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normal arts\n"
     ]
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mZeroDivisionError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[49], line 8\u001B[0m\n\u001B[1;32m      6\u001B[0m     indexation1 \u001B[38;5;241m=\u001B[39m score_normal_pagerank(q, pageranks[topic_1])\n\u001B[1;32m      7\u001B[0m     indexation2 \u001B[38;5;241m=\u001B[39m score_normal_pagerank(q, pageranks[topic_2])\n\u001B[0;32m----> 8\u001B[0m     sim \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[43mKSim\u001B[49m\u001B[43m(\u001B[49m\u001B[43mindexation1\u001B[49m\u001B[43m[\u001B[49m\u001B[43m:\u001B[49m\u001B[38;5;241;43m20\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mindexation2\u001B[49m\u001B[43m[\u001B[49m\u001B[43m:\u001B[49m\u001B[38;5;241;43m20\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      9\u001B[0m sim \u001B[38;5;241m/\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlen\u001B[39m(queries)\n\u001B[1;32m     10\u001B[0m table2 \u001B[38;5;241m=\u001B[39m {\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtopic_1\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtopic_2\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m: sim}\n",
      "Cell \u001B[0;32mIn[23], line 16\u001B[0m, in \u001B[0;36mKSim\u001B[0;34m(t1, t2)\u001B[0m\n\u001B[1;32m     14\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m np\u001B[38;5;241m.\u001B[39msign(t1_prime\u001B[38;5;241m.\u001B[39mindex(u)\u001B[38;5;241m-\u001B[39mt1_prime\u001B[38;5;241m.\u001B[39mindex(v))\u001B[38;5;241m==\u001B[39mnp\u001B[38;5;241m.\u001B[39msign(t2_prime\u001B[38;5;241m.\u001B[39mindex(u)\u001B[38;5;241m-\u001B[39mt2_prime\u001B[38;5;241m.\u001B[39mindex(v)):\n\u001B[1;32m     15\u001B[0m         sim\u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m\n\u001B[0;32m---> 16\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43msim\u001B[49m\u001B[38;5;241;43m/\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mlen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mU\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mlen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mU\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m-\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mZeroDivisionError\u001B[0m: division by zero"
     ]
    }
   ],
   "execution_count": 49
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
